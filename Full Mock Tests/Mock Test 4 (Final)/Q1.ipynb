{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d32a949",
   "metadata": {},
   "source": [
    "# AIO Q1 -- Simplified Spiking Neural Networks\n",
    "In this question we will implement a simplified version of the \"biologically-plausible\" Leaky-Integrate-and-Fire (LIF) Spiking Neuron and do some simple tasks with it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df94f671",
   "metadata": {},
   "source": [
    "## Dependencies and Dataset\n",
    "Here is some setup and dependencies you'll be using. You may import other libraries if you so choose, though not any that directly implement LIF Neurons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8fa89529",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dependencies\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c70d932c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Loading MNIST dataset from torchvision.datasets\n",
    "from torchvision import datasets, transforms\n",
    "\n",
    "dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "plt.imshow(dataset[0][0].squeeze(), cmap='gray')\n",
    "plt.title(f\"Label: {dataset[0][1]}\")\n",
    "plt.show()\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "021e437c",
   "metadata": {},
   "source": [
    "## 1 Bernoulli encoding (5): \n",
    "In part 1 and 2 we will create code to encode one an input into a timeseries (in particular a spike-train) of ones and zeros.\n",
    "</br></br>\n",
    "First, we will encode using samples from the bernoulli distribution. To do this we will treat each pixel, $x_{i,j} \\in (0,1)$, as a probability. We will create a new tensor from it with an additional dimension, time. At each time, t, this output $\\hat{x}_{i,j,t}$ will be 1 with probability $x_{i,j} * prob\\_scale\\_factor$ where prob scale factor will be a hyperparameter between 0 and 1, and will be 0 otherwise.That is, the output will be a binary tensor, $\\hat{x} \\in [0,1]^{28,28,T}$ in the case of MNIST.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416dd508",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bernoulli_encoding(x, prob_scale_factor, timesteps=150):\n",
    "    # Convert input tensor x to Bernoulli encoded tensor\n",
    "    # You may use torch.bernoulli for this purpose\n",
    "    # The function should be the same shape as the input tensor x, with an additional dimension for timesteps\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99159e46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31013b2",
   "metadata": {},
   "source": [
    "## 2 Poisson-Rate Encoding (20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701db083",
   "metadata": {},
   "source": [
    "### 2a Poisson Distribution (10):\n",
    "Next we will do a rate encoding based off of the Poisson distribution which, given a rate will predict the probability of any number of occurences, k (always an integer), happening. The probability density function of the poisson distribution is given as a function of the rate parameter, $\\lambda$, below: \n",
    "$$\\begin{align*}\n",
    "P(\\hat{x} = k) = \\frac{\\lambda^k * e^{-\\lambda}}{k!}\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c840d8b",
   "metadata": {},
   "source": [
    "*Question:* Prove that $E[\\hat{x}] = Var[\\hat{x}] = \\lambda$ :"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c86dac0",
   "metadata": {},
   "source": [
    "### 2b Rate Encoding (10)\n",
    "Using this as a basis, we will convert the values of the input, x, to samples drawn from the poisson distibution over T timesteps, $\\hat{x}$, using the pixel values of x as our rate parameter, i.e:\n",
    "$$\\begin{align*}\n",
    "P(\\sum^T_{t=0}\\hat{x}_{i,j,t} = k) = \\frac{T*x_{i,j}^k * e^{-x_{i,j}}}{k!}\n",
    "\\end{align*}$$\n",
    "Where k is an integer corresponding to the number of spikes.\n",
    "</br></br>\n",
    "We will first produce a 2d Tensor of $k_{i,j} \\in (0, T)$ samples, which we will convert to a 3d binary Tensor, $\\hat{x}$ where $\\sum^T_{t=0}\\hat{x}_{i,j,t} = k$ and the ones are evenly spaced along the time dimension. That is there should be a 1 in every other $\\frac{k_{i,j}}{T}$ places of $\\hat{x}_{i,j,t}$ along the time dimension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94f254d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rate_encoding(x, scaling_factor, timesteps=150):\n",
    "    # Convert input tensor x * scaling_factor to Rate encoded tensor -- if k > T for any input pixel, set k = T\n",
    "    # The function should be the same shape as the input tensor x, with an additional dimension for timesteps\n",
    "    # You may use torch.poisson\n",
    "    \n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7b11db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test on MNIST example"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0f080d9",
   "metadata": {},
   "source": [
    "## 3. A Simple Spiking Neuron (10)\n",
    "We will, for the course of this question, consider a simplified version of the Leaky Integrate-and-Fire (LIF) neuron. We will define it like so:\n",
    "</br></br>\n",
    "$V_t$ : The internal voltage at time t\n",
    "</br>\n",
    "$\\tau_t$ : the firing threshold (internal voltage threshold) of the neuron (at time t -- as it increases as the neuron fires)\n",
    "</br>\n",
    "$\\tau_+$ : the amount the firing threshold increases when the neuron fires.\n",
    "</br>\n",
    "$O_t$ : the output, either 0 or 1 at timestep t (whether the neuron fires at that time or not)\n",
    "</br>\n",
    "$d$ : The decay rate of the internal voltage\n",
    "</br></br>\n",
    "Given some input X, over timesteps, t (0 -> T), the internal voltage, threshold and output will be updated like so:\n",
    "$$V_t = \\begin{cases}\n",
    "d*V_{t-1} + X_t \\quad if & d*V_{t-1} + X_t < \\tau \\\\\n",
    "0 \\quad & otherwise \n",
    "\\end{cases}\n",
    "$$\n",
    "$$\\tau_t = \\begin{cases}\n",
    "\\tau_{t-1} \\quad if & d*V_{t-1} + X_t < \\tau \\\\\n",
    "\\tau_{t-1} + \\tau_+ \\quad & otherwise \n",
    "\\end{cases}\n",
    "$$\n",
    "$$O_t = \\begin{cases}\n",
    "0 \\quad if & d*V_{t-1} + X_t < \\tau \\\\\n",
    "1 \\quad & otherwise \n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eee3a44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement a Spiking Neuron as described above and test it on a sample input\n",
    "# You may use whatever library you'd like\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecaed5cb",
   "metadata": {},
   "source": [
    "## 4. Theoretical Spiking Neuron Power (10):\n",
    "For the following two parts consider the following task:\n",
    "</br></br>\n",
    "Given some input, $X \\in [0,1]^n$, return 1 if and only if there are three ones in sequence in X. I.e. [1,1,1,0,0] -> 1 and [1,1,0,1,1] -> 0\n",
    "</br></br>\n",
    "a. Prove that a single spiking neuron (as described above) is able to complete this task perfectly. \n",
    "</br>\n",
    "(note: X is able to be given over time and the output may be taken as an OR over all timesteps)\n",
    "</br></br>\n",
    "Answer:\n",
    "</br></br>\n",
    "b. Prove that this task is not linearly separable (equivelantly there is no single ANN layer + threshold which can represent this). \n",
    "</br></br>\n",
    "Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9f4ae04",
   "metadata": {},
   "source": [
    "## 5. Weighted SNN (10)\n",
    "Next we will add learnable parameters and make a network. This will be based off of they idea of connection strength (thick axon) between real neurons."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4c2ba41",
   "metadata": {},
   "source": [
    "We will create an array of 100 of our Spiking Neurons from 3, in addition, we will also include a 784 x 100 weight matrix. Now, instead of directly updating the internal voltages by adding X we will instead add W*X. That is, for neuron n of our model the internal voltage will now be updated like so:\n",
    "$$V_{n,t} = \\begin{cases}\n",
    "d*V_{n, t-1} + \\sum_{i,j} w_{n,i,j} * X_{i,j,t} \\quad if & \\text{not spiking} \\\\\n",
    "0 \\quad & otherwise \n",
    "\\end{cases}\n",
    "$$\n",
    "Below, implement this setup in such a way that you can input spike trains into it and produce output spikes. Weight should be initialized to a gaussian with mean = 1, var = 0.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abbe144f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement the weighted SNN as above and test it on a single input from MNIST\n",
    "# both Bernoulli and Rate encoding should be tested"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e7a338",
   "metadata": {},
   "source": [
    "## 6. Learning Rules (15)\n",
    "Here we will now implement 2 different learning rules for spiking neurons. Hebbian learning and Spike-Timing Dependent Plasticity (STDP). Both will use the following to determine weight change:\n",
    "$$\\Delta t = t_{out} - t_{in} $$\n",
    "where $t_{in}$ is the timestep of the most recent input spike and $t_{out}$ is the spike made by the recieving neuron (who's weights we are updating).  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc688bd4",
   "metadata": {},
   "source": [
    "### 6a. Hebbian Learning (5)\n",
    "Hebbian Learning is named after Neurologist Donald Hebb. His theory is often summarized as \"Neurons that fire together, wire together\". As such the learning rule will increase weights if this $\\Delta t$ is sufficiently small. In particular the weight change will be given as:\n",
    "$$\\Delta W = \n",
    "\\begin{cases}\n",
    "dw_+ \\quad if & |\\Delta t| \\leq \\tau_{Hebb} \\\\\n",
    "dw_- \\quad    &  otherwise\n",
    "\\end{cases}$$\n",
    "Where $dw_+$ is the weight change if the neurons fire together and $dw_-$ is the (generally lower magnitude, sometimes zero) change when the recieving neuron fires too far away from the input neuron. $\\tau_{Hebb}$ is the threshold of whether or not the neurons fired together.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0208e0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement hebbian learning rule.\n",
    "def hebbian_update(weights, pre_spikes, post_spikes, w_plus, w_minus, tau_hebb):\n",
    "\n",
    "    pass\n",
    "\n",
    "# Plot your hebbian weight updates versus time difference\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d001f5",
   "metadata": {},
   "source": [
    "### 6b. Spike Timing dependent Plasticity (10)\n",
    "Now we will consider the case where a negative $\\Delta t$ is considered opposite the positive case. Further this will also update dependent on the magnitude of $\\Delta t$. \n",
    "</br>\n",
    "Particularly the update via STDP will be:\n",
    "$$\\Delta W =\n",
    "\\begin{cases}\n",
    "A_+ * exp(\\frac{\\Delta t}{\\delta t_{+max}}) \\quad & if \\quad \\Delta t > 0 \\\\\n",
    "- A_- * exp(- \\frac{\\Delta t}{\\delta t_{-max}}) \\quad & otherwise\n",
    "\\end{cases}$$\n",
    "Where $A_+$ is the max positive weight change, $A_-$ is the max negative weight change, $\\delta t_{+max}$ is the maximum positive time difference, and $\\delta t_{-max}$ is the maximum negative time difference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d7bc77a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement STDP and plot Delta t vs Delta W -- There should be a discontinuity at Delta t = 0 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6d2bb5",
   "metadata": {},
   "source": [
    "## 7. Model test (10)\n",
    "Now we will put everything together and see how it works. Note this is unsupervised learning, to measure the accuracy of classification we will have to measure the consistency of the spiking (that is what proportion of the inputs a neuron spiked for from the class it spikes for most frequently).\n",
    "\n",
    "### 7a. Approximate Accuracy\n",
    "First, implement the accuracy measuring method described above. It should take as an input a series of spike trains (output trains from each neuron over many inputs) and labels and determine the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87ade3bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement accuracy measurement here -- then test on your untrained model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d03d9a",
   "metadata": {},
   "source": [
    "### 7b. Training loop\n",
    "Now we will train our weighted model. Write a training loop and test on 1 epoch of training via STDP. Report the accuracy curve over the course of the epoch at intervals of 500 inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e43984aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop here\n",
    "# Note, internal voltages should be reset in-between inputs, though voltage thresholds should not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88a295ff",
   "metadata": {},
   "source": [
    "## 8. Model Improvement (20)\n",
    "We will now make adjustments to our model to improve the performance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a38208",
   "metadata": {},
   "source": [
    "### 8a. Winner-take-all (10)\n",
    "Winner-take-all models essentially act as a maximization over the output. For our Spiking Neural Network this means that once one neuron spikes no others will be allowed to for that input, we can also soften this to allow all neurons to spike until one neuron spikes m times.\n",
    "</br></br>\n",
    "You must also plot your weights such that each neuron's corresponding weights are shown as 28 x 28 blocks of a large 2800 x 2800 image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed3d023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Implement a winner-take-all single-layer SNN as described above. Report the accuracy curve and plot the weights before and after training one epoch.\n",
    "# Also plot the weights of your non winner-take-all model before and after training for comparison."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "274c9788",
   "metadata": {},
   "source": [
    "### 8b. Open-ended improvement (10)\n",
    "Using whatever method you would like to modify your model (particularly hyperparameters), achieve 85% estimated accuracy via unsupervised learning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412f0603",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
