{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Km5x5gG9sLQL"
   },
   "source": [
    "# AIO Q4 — Principal Component Analysis (PCA)\n",
    "\n",
    "## What is PCA?\n",
    "\n",
    "**Principal Component Analysis (PCA)** is a dimensionality reduction technique that finds the directions of maximum variance in high-dimensional data. Think of it as finding the \"most important\" axes to represent your data with fewer dimensions while losing as little information as possible.\n",
    "\n",
    "**Why does this matter?**\n",
    "- **Visualization:** Reduce 100-dimensional data to 2D for plotting\n",
    "- **Noise reduction:** Remove low-variance dimensions that are mostly noise\n",
    "- **Compression:** Store data more efficiently\n",
    "- **Feature extraction:** Find meaningful combinations of original features\n",
    "\n",
    "---\n",
    "\n",
    "## Notation\n",
    "\n",
    "Throughout this problem, we use the following notation:\n",
    "\n",
    "| Symbol | Meaning |\n",
    "|--------|---------|\n",
    "| $n$ | number of data points |\n",
    "| $d$ | dimension of each data point |\n",
    "| $k$ | number of principal components to retain |\n",
    "| $X \\in \\mathbb{R}^{n \\times d}$ | data matrix (each row is a data point) |\n",
    "| $\\Sigma \\in \\mathbb{R}^{d \\times d}$ | covariance matrix |\n",
    "| $\\lambda_1 \\ge \\lambda_2 \\ge ... \\ge \\lambda_d$ | eigenvalues in descending order |\n",
    "| $v_1, v_2, ..., v_d$ | corresponding eigenvectors |\n",
    "| $I_d$ | $d \\times d$ identity matrix |\n",
    "\n",
    "---\n",
    "\n",
    "## Key Definitions\n",
    "\n",
    "**Covariance Matrix:** A matrix $\\Sigma$ where entry $\\Sigma_{ij}$ measures how much features $i$ and $j$ vary together. For centered data $\\tilde{X}$: $\\Sigma = \\frac{1}{n}\\tilde{X}^T\\tilde{X}$\n",
    "\n",
    "**Frobenius Norm:** For a matrix $A$, the Frobenius norm is: $\\|A\\|_F = \\sqrt{\\sum_{i,j} A_{ij}^2} = \\sqrt{\\text{tr}(A^T A)}$\n",
    "\n",
    "**Trace:** The trace of a square matrix is the sum of its diagonal entries: $\\text{tr}(A) = \\sum_{i=1}^{d} A_{ii}$\n",
    "\n",
    "**Orthonormal Vectors:** A set of vectors $\\{v_1, ..., v_k\\}$ is orthonormal if each vector has unit length ($\\|v_i\\|_2 = 1$) and all pairs are perpendicular ($v_i^T v_j = 0$ for $i \\neq j$). Equivalently, $V^T V = I$ where $V$ has these vectors as columns.\n",
    "\n",
    "**Principal Components:** The eigenvectors of the covariance matrix $\\Sigma$, ordered by their eigenvalues. The first principal component $v_1$ points in the direction of maximum variance in the data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wHRFwzvVsJF9"
   },
   "source": [
    "Setup Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 2574,
     "status": "ok",
     "timestamp": 1765696357325,
     "user": {
      "displayName": "Mimi Lertsaroj",
      "userId": "06740988647120056794"
     },
     "user_tz": 360
    },
    "id": "HCPCFiRtT81H"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_iris\n",
    "\n",
    "# Random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "# Load Iris dataset\n",
    "iris = load_iris()\n",
    "iris_data = iris.data  # Shape: (150, 4)\n",
    "iris_labels = iris.target  # Shape: (150,)\n",
    "iris_feature_names = iris.feature_names  # ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
    "iris_target_names = iris.target_names  # ['setosa', 'versicolor', 'virginica']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1cnvgFZb9kfX"
   },
   "source": [
    "## Q1 — Covariance Matrix Fundamentals (6 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bDMqbDG1UTZy"
   },
   "source": [
    "Consider a dataset $X \\in \\mathbb{R}^{n \\times d}$ where each row $x_i^T$ represents a data point.\n",
    "\n",
    "**Part (a):** Write the expression for the mean vector $\\hat{\\mu}$ (the average of all rows of $X$) and the centered data matrix $\\tilde{X} \\in \\mathbb{R}^{n \\times d}$ (data with mean subtracted). (2 points)\n",
    "\n",
    "```\n",
    "Answer (a):\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "**Part (b):** Prove that the sample covariance matrix can be written as: (2 points)\n",
    "\n",
    "$$\\Sigma = \\frac{1}{n}\\tilde{X}^T\\tilde{X}$$\n",
    "\n",
    "```\n",
    "Answer (b):\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bzsu1p13T7qs"
   },
   "source": [
    "## Q2 — Computing Principal Components by Hand (7 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X62L0UKs9zVN"
   },
   "source": [
    "Consider the following dataset, represented as three points in $\\mathbb{R}^2$. Assume the data is already centered (zero mean).\n",
    "\n",
    "$$X = \\begin{bmatrix} 1 & 2 \\\\ 1.5 & 3 \\\\ 6 & 12 \\end{bmatrix}$$\n",
    "\n",
    "The **principal components** are the eigenvectors of the covariance matrix $\\Sigma = \\frac{1}{n}X^T X$, ordered by eigenvalue magnitude."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JXnq7k1mNIok"
   },
   "source": [
    "**Part (a):** What is the first principal component vector $v_1$? (2 points)\n",
    "\n",
    "```\n",
    "Answer (a):\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "**Part (b):** What is the second principal component $v_2$? (2 points)\n",
    "\n",
    "```\n",
    "Answer (b):\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "**Part (c):** Project the data onto the first principal component by computing $X v_1$ (compressing from 2D to 1D). What is the 1D representation of each point? (2 points)\n",
    "\n",
    "```\n",
    "Answer (c):\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "**Part (d):** Will this representation be lossy, or will it perfectly preserve the data? Explain why. (2 points)\n",
    "\n",
    "```\n",
    "Answer (d):\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ks_3ga_hF3Dg"
   },
   "source": [
    "## Q3 — Structured Covariance Matrix (4 points) — *Optional/Bonus*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xp9uH_CrYlNo"
   },
   "source": [
    "Consider a covariance matrix $\\Sigma$ of the form:\n",
    "\n",
    "$$\\Sigma = \\gamma I_p + aa^T$$\n",
    "\n",
    "where $\\gamma > 0$ is a scalar, $I_p$ is the $p \\times p$ identity matrix, and $a$ is a vector of dimension $p$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G6_n3xkDQb2z"
   },
   "source": [
    "**Part (a):** Show that $a$ is an eigenvector of $\\Sigma$. What is its eigenvalue? (2 points)\n",
    "\n",
    "```\n",
    "Answer (a):\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "**Part (b):** Show that if $b$ is any vector such that $a^T b = 0$, then $b$ is also an eigenvector of $\\Sigma$. What is the eigenvalue corresponding to $b$? (2 points)\n",
    "\n",
    "```\n",
    "Answer (b):\n",
    "\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rs39QRG-AOA0"
   },
   "source": [
    "## Q4 — SVD Properties and Derivation (15 points) — *Theory*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XUwGvDfEAcRY"
   },
   "source": [
    "Consider any matrix $A \\in \\mathbb{R}^{n \\times d}$.\n",
    "\n",
    "**Part (a):** Let $v$ be a unit norm eigenvector of $A^T A$ with eigenvalue $\\lambda$. Prove that $\\lambda \\geq 0$ and that $\\|Av\\|_2 = \\sqrt{\\lambda}$. (2 points)\n",
    "\n",
    "```\n",
    "Answer (a):\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "**Part (b):** Let $v_1, v_2$ be two eigenvectors of $A^T A$ that are orthogonal to each other and correspond to non-zero eigenvalues. Prove that $Av_1$ and $Av_2$ are also orthogonal to each other. (2 points)\n",
    "\n",
    "```\n",
    "Answer (b):\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "**Part (c):** Let $V \\in \\mathbb{R}^{d \\times d}$ contain the $d$ eigenvectors of $A^T A$ as its columns and let $\\Lambda \\in \\mathbb{R}^{d \\times d}$ contain their corresponding eigenvalues. Assume $A^T A$ is full rank. Prove that $U = AV\\Lambda^{-1/2}$ has orthonormal columns. (2 points)\n",
    "\n",
    "*Hint: Apply parts (a) and (b).*\n",
    "\n",
    "```\n",
    "Answer (c):\n",
    "\n",
    "\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3nJXr8bPQ7R-"
   },
   "source": [
    "## Q5 — Optimal Low-Rank Approximation (18 points) — *Theory*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qoc0AmQbYuEr"
   },
   "source": [
    "In this problem, you will prove that PCA provides the **optimal low-rank approximation** by showing that projecting onto the top $k$ principal components minimizes reconstruction error.\n",
    "\n",
    "Consider any matrix $A \\in \\mathbb{R}^{n \\times d}$.\n",
    "\n",
    "**Convention:** Throughout this problem, let $Z \\in \\mathbb{R}^{d \\times k}$ be a matrix with orthonormal columns (i.e., $Z^T Z = I_k$). All min/max operations are over such matrices $Z$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o-DeVV0AS-YT"
   },
   "source": [
    "**Part (a):** Prove that: \n",
    "\n",
    "$$\\arg\\min_{Z} \\|A - AZZ^T\\|_F^2 = \\arg\\max_{Z} \\text{tr}(Z^T A^T A Z)$$\n",
    "\n",
    "```\n",
    "Answer (a):\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "**Part (b):** Let $V \\in \\mathbb{R}^{d \\times d}$ have orthonormal columns. Prove that for any $Z \\in \\mathbb{R}^{d \\times k}$ with orthonormal columns, $V^T Z$ also has orthonormal columns. Further prove that any such $Z$ can be written as $Z = VU$ for some $U \\in \\mathbb{R}^{d \\times k}$ with orthonormal columns. (2 points)\n",
    "\n",
    "```\n",
    "Answer (b):\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "**Part (c):** Writing $A^T A = V\\Lambda V^T$ in its eigendecomposition, use part (b) to prove that: (2 points)\n",
    "\n",
    "$$\\max_{Z} \\text{tr}(Z^T A^T A Z) = \\max_{Z} \\text{tr}(Z^T \\Lambda Z)$$\n",
    "\n",
    "```\n",
    "Answer (c):\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "**Part (d):** Prove that for $Z \\in \\mathbb{R}^{d \\times k}$ with orthonormal columns: $\\text{tr}(ZZ^T) = k$ and $0 \\leq (ZZ^T)_{i,i} \\leq 1$ for all $i \\in [d]$. (2 points)\n",
    "\n",
    "```\n",
    "Answer (d):\n",
    "\n",
    "\n",
    "```\n",
    "\n",
    "**Part (e):** Use parts (c) and (d) to show that $\\max_{Z} \\text{tr}(Z^T A^T A Z) = \\sum_{i=1}^k \\lambda_i(A^T A)$. Conclude that the optimal $Z$ has as its columns the top $k$ eigenvectors of $A^T A$. (2 points)\n",
    "\n",
    "```\n",
    "Answer (e):\n",
    "\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C-3u7CgCXcSk"
   },
   "source": [
    "## Q6 — Implementing BasicPCA (15 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cxdmhy9Ge8qA"
   },
   "source": [
    "In this problem, you will implement the class `BasicPCA` using eigendecomposition of the covariance matrix.\n",
    "\n",
    "Implement the class according to the specifications below. Include shape comments after each operation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tQFKOz4sjNnj"
   },
   "source": [
    "Attributes:\n",
    "\n",
    "* `n_components`: Number of principal comonents to retain\n",
    "* `mean_`: Mean vector of training data, shape `(d,)`\n",
    "* `components_`: Principal component vectors, shape `(n_components, d)` where each row is a principal component\n",
    "* `explained_variance_`: Variance explained by each component, shape `(n_components,)`\n",
    "* `explained_variance_ratio_`: Proportion of total variance explained by each component, shape `(n_components,)`\n",
    "\n",
    "Method `__init__`:\n",
    "* Inputs:\n",
    "  * `n_components`: Integer specifying number of components\n",
    "* Outputs:\n",
    "  * None\n",
    "* What to do:\n",
    "  * Initialise all attributes\n",
    "\n",
    "Method `fit`:\n",
    "* Inputs:\n",
    "  * `X`: Training data with shape `(n,d)`, where n is number of samples and d is number of features\n",
    "* Outputs:\n",
    "  * `self`\n",
    "* What to do:\n",
    "  * Compute mean vector and centre the data\n",
    "  * Compute covariance matrix\n",
    "  * Compute eigendecomposition using `np.linalg.eigh` (for symmetric matrices)\n",
    "  * Sort eigenvalues and eigenvectors in descending order\n",
    "  * Store the top `n_components` eigenvectors as principal components (as rows)\n",
    "  * Store corresponding eigenvalues as explained variance\n",
    "  * Compute explained variance ratios\n",
    "  * After each operation, add a comment on the tensor shape\n",
    "  * DO NOT use any loop for main computations\n",
    "\n",
    "\n",
    "Method `transform`:\n",
    "* Inputs:\n",
    "  * `X`: Data to transform, shape `(n,d)`\n",
    "* Outputs:\n",
    "  *`X_transformed`: Projected data, shape `(n,n_components)`\n",
    "* What to do:\n",
    "  * Centre the input data using the stored mean\n",
    "  * Project onto principal components\n",
    "\n",
    "Method `fit_transform`:\n",
    "* Inputs:\n",
    "  * `X`: Data to transform, shape `(n,d)`\n",
    "* Outputs:\n",
    "  *`X_transformed`: Projected data, shape `(n,n_components)`\n",
    "* What to do:\n",
    "  * Call fit and transform in sequence\n",
    "\n",
    "Method `inverse_transform`:\n",
    "* Inputs:\n",
    "  * `X_transformed`: Data in PC space, shape `(n,n_components)`\n",
    "* Outputs:\n",
    "  * `X_reconstructed`: Reconstructed data in original space, shape `(n,d)`\n",
    "* What to do:\n",
    "  * Project back from PC space to original space\n",
    "  * Add back the mean\n",
    "  * Add shape comments\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "neMxjtsAd7y2"
   },
   "source": [
    "## Q7 — Implementing SVDPCA (15 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0nGpwvx4zh_i"
   },
   "source": [
    "In this problem, you will build your own PCA class `SVDPCA` using Singular Value Decomposition (SVD) computed via eigendecomposition.\n",
    "\n",
    "Implement the class according to the specifications below. Include shape comments after each operation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PL1gA8zXzx8q"
   },
   "source": [
    "Attributes:\n",
    "\n",
    "* `n_components`: Number of principal comonents to retain\n",
    "* `mean_`: Mean vector of training data, shape `(d,)`\n",
    "* `components_`: Principal component vectors, shape `(n_components, d)` where each row is a principal component\n",
    "* `singular_values_`: Singular values from SVD, shape `(n_components,)`\n",
    "* `explained_variance_`: Variance explained by each component, shape `(n_components,)`\n",
    "* `explained_variance_ratio_`: Proportion of total variance explained by each component, shape `(n_components,)`\n",
    "\n",
    "Method `__init__`:\n",
    "* Inputs:\n",
    "  * `n_components`: Integer specifying number of components\n",
    "* Outputs:\n",
    "  * None\n",
    "* What to do:\n",
    "  * Initialise all attributes\n",
    "\n",
    "Method `fit`:\n",
    "* Inputs:\n",
    "  * `X`: Training data with shape `(n,d)`, where n is number of samples and d is number of features\n",
    "* Outputs:\n",
    "  * `self`\n",
    "* What to do:\n",
    "  * Compute mean vector and centre the data\n",
    "  * Manually compute $\\mathbf{SVD}$ using eigendecomposition:\n",
    "    * For the centred data matrix $X\\in\\mathbb{R}^{n\\times d}$, we want $X = U \\Sigma V^T$\n",
    "    * Use the fact that $V$ contains eigenvectors of $X^TX$ and $\\Sigma^2$ contains eigen values of $X^TX$\n",
    "    * Compute $A=X^TX$ (shape: $d\\times d$)\n",
    "    * Compute the eigendecomposition of $A$ using `np.linalg.eigh`: $A = V\\Lambda V^T$\n",
    "    * Sort eigenvalues and eigenvectors in descending order\n",
    "    * Extract singular values: $\\sigma_i = \\sqrt{\\lambda}_i$ for $i = 0, ..., d-1$\n",
    "    * The right singular vectors are $V$ (eigenvectors of $X^TX$)\n",
    "    * Note: Principal components are rows of $V^T$, which are columns of $V$\n",
    "  * Store the first `n_components` rows of `V^T` as principal components\n",
    "  * Store corresponding singular values\n",
    "  * Compute explained variance from singular values using: variance = $\\sigma^2/(n)$\n",
    "  * Compute explained variance ratios\n",
    "  * Do NOT use any loop for main computation\n",
    "\n",
    "Method `transform`:\n",
    "* Inputs:\n",
    "  * `X`: Data to transform, shape `(n,d)`\n",
    "* Outputs:\n",
    "  *`X_transformed`: Projected data, shape `(n,n_components)`\n",
    "* What to do:\n",
    "  * Centre the input data using the stored mean\n",
    "  * Project onto principal components\n",
    "\n",
    "Method `fit_transform`:\n",
    "* Inputs:\n",
    "  * `X`: Data to transform, shape `(n,d)`\n",
    "* Outputs:\n",
    "  * `X_transformed`: Projected data, shape `(n,n_components)`\n",
    "* What to do:\n",
    "  * Call fit and transform in sequence\n",
    "\n",
    "Method `inverse_transform`:\n",
    "* Inputs:\n",
    "  * `X_transformed`: Data in PC space, shape `(n,n_components)`\n",
    "* Outputs:\n",
    "  * `X_reconstructed`: Reconstructed data in original space, shape `(n,d)`\n",
    "* What to do:\n",
    "  * Project back from PC space to original space\n",
    "  * Add back the mean\n",
    "  * Add shape comments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4z1TMkKfeM9d"
   },
   "source": [
    "## Q8 — Testing PCA on Iris Dataset (12 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ca3CAIacUSSl"
   },
   "source": [
    "In this problem, you will test both PCA implementations on the Iris dataset and verify their correctness.\n",
    "\n",
    "**Note:** The Iris dataset has already been loaded in the setup code:\n",
    "- `iris_data`: shape (150, 4) — the feature matrix\n",
    "- `iris_labels`: shape (150,) — the class labels\n",
    "- `iris_feature_names`: list of 4 feature names\n",
    "- `iris_target_names`: list of 3 class names\n",
    "\n",
    "---\n",
    "\n",
    "**Part (a): Implement the test function** (4 points)\n",
    "\n",
    "Define a function `test_pca_on_iris()` that:\n",
    "- Sets `n_components = 2`\n",
    "- Fits both `BasicPCA` and `SVDPCA` on the iris data\n",
    "- Transforms the iris data using both models\n",
    "- Prints: original data shape, transformed data shapes, explained variance ratios, total variance explained\n",
    "- Does NOT use any loops for main computations\n",
    "\n",
    "---\n",
    "\n",
    "**Part (b): Compare the implementations** (3 points)\n",
    "\n",
    "In the same function, verify both implementations produce equivalent results by computing:\n",
    "1. Maximum absolute difference between principal components (accounting for sign flips)\n",
    "2. Maximum absolute difference between explained variance ratios\n",
    "3. Maximum absolute difference between transformed data (accounting for sign flips)\n",
    "4. Print whether methods agree within tolerance 1e-10\n",
    "\n",
    "*Hint: Components can be flipped in sign. Check both |v1 - v2| and |v1 + v2| and take the minimum.*\n",
    "\n",
    "---\n",
    "\n",
    "**Part (c): Reconstruction error** (3 points)\n",
    "\n",
    "In the same function, compute and print:\n",
    "1. Reconstruction error for both models: $\\|X - X_{reconstructed}\\|_F^2$\n",
    "2. Verify reconstruction errors match between models\n",
    "\n",
    "After implementing, call: `test_pca_on_iris()`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q9 - Visualising PCA on Iris (12 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this part you will visualise the PCA results on the Iris dataset.\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "Part (a): Create a scatter plot function\n",
    "\n",
    "Define a function `plot_pca_results(X_transformed, labels, title, explained_variance_ratio)` that:\n",
    "\n",
    "Inputs:\n",
    "* `X_transformed`: Transformed data in 2D, shape `(n, 2)`\n",
    "* `labels`: Class labels, shape `(n,)`\n",
    "* `title`: String for plot title\n",
    "* `explained_variance_ratio`: Array of explained variance ratios for the 2 components, shape `(2,)`\n",
    "\n",
    "Outputs:\n",
    "* None \n",
    "\n",
    "What to do inside this function:\n",
    "1. Create a figure with size `(10, 8)`\n",
    "2. Create a scatter plot with `X_transformed[:, 0]` on x-axis and `X_transformed[:, 1]` on y-axis\n",
    "3. Color points by their class label (use 3 different colors for 3 classes)\n",
    "4. Add labels: \"PC1 (X.X% variance)\" for x-axis, \"PC2 (X.X% variance)\" for y-axis, where X.X is the percentage from `explained_variance_ratio`\n",
    "5. Add a legend showing \"Setosa\", \"Versicolor\", \"Virginica\"\n",
    "6. Add grid for better readability\n",
    "7. Set the title\n",
    "8. Use plt.show() to display the plot\n",
    "9. Do not use any loop for plotting (matplotlib can handle arrays directly)\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "Part (b): Visualise explained variance\n",
    "\n",
    "Define a function `plot_explained_variance(explained_variance_ratio) that`:\n",
    "\n",
    "Inputs:\n",
    "* `explained_variance_ratio`: Array of explained variance ratios for all 4 components, shape `(4,)`\n",
    "\n",
    "Outputs:\n",
    "* None\n",
    "\n",
    "What to do:\n",
    "\n",
    "1. Create a figure with size (10, 6)\n",
    "2. Compute cumulative explained variance: `cumulative_variance = np.cumsum(explained_variance_ratio)`\n",
    "3. Create a bar plot showing explained variance ratio for each component (use component numbers 1, 2, 3, 4 on x-axis)\n",
    "4. On the same plot, add a line plot showing cumulative explained variance with markers\n",
    "5. Label x-axis as \"Principal Component\"\n",
    "6. Label y-axis as \"Explained Variance Ratio\"\n",
    "7. Add a legend distinguishing between \"Individual Explained Variance\" and \"Cumulative Explained Variance\"\n",
    "8. Add grid with transparency (alpha=0.3)\n",
    "9. Set title as \"PCA Explained Variance Analysis\"\n",
    "10. Use `plt.tight_layout()` to prevent label overlap\n",
    "11. Use plt.show() to display the plot (do NOT save to file)\n",
    "12. Do not use any loop for plotting\n",
    "\n",
    "\n",
    "---\n",
    "Part (c): Call the functions\n",
    "\n",
    "After implementing both functions, create and display the plots.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code here"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyMCSj2EfqZSj+VoDSxLJku+",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
