{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "831e9007",
   "metadata": {},
   "source": [
    "# AIO Q3 — Simple Attention\n",
    "\n",
    "We study a two-input attention-style model:\n",
    "\n",
    "$$\n",
    "\\alpha_i = \\frac{e^{\\beta_i}}{e^{\\beta_1} + e^{\\beta_2}}, \\quad\n",
    "y = \\alpha_1 x_1 + \\alpha_2 x_2, \\quad\n",
    "L = \\tfrac{1}{2}(y - t)^2\n",
    "$$\n",
    "\n",
    "Constants (used unless stated otherwise):  \n",
    "$x_1 = 2,\\; x_2 = 4,\\; t = 3$  \n",
    "Logits for forward pass: $\\beta_1 = 0,\\; \\beta_2 = 1$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8579b91e",
   "metadata": {},
   "source": [
    "## Q1 — Forward: Exponentials\n",
    "\n",
    "Compute $e^{\\beta_1}$ and $e^{\\beta_2}$ for $\\beta_1 = 0,\\; \\beta_2 = 1$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c40b532a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ec4377b7",
   "metadata": {},
   "source": [
    "## Q2 — Forward: Softmax Weights\n",
    "\n",
    "Using your results from Q1, compute\n",
    "\n",
    "$$\n",
    "\\alpha_i = \\frac{e^{\\beta_i}}{e^{\\beta_1} + e^{\\beta_2}}, \\quad i \\in \\{1, 2\\}.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef21e6df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "240abded",
   "metadata": {},
   "source": [
    "## Q3 — Forward: Model Output\n",
    "\n",
    "Given $x_1 = 2,\\; x_2 = 4$, compute\n",
    "\n",
    "$$\n",
    "y = \\alpha_1 x_1 + \\alpha_2 x_2.\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95a99ac2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "64ef2f61",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Q4 — Derivatives w.r.t. Inputs\n",
    "\n",
    "Show that\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial x_i} = (y - t)\\,\\alpha_i.\n",
    "$$\n",
    "\n",
    "*Hint:* Chain rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64257849",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "efbe6685",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Q5 — Softmax Derivative Formula\n",
    "\n",
    "Differentiate\n",
    "\n",
    "$$\n",
    "\\alpha_i = \\frac{e^{\\beta_i}}{e^{\\beta_1} + e^{\\beta_2}}\n",
    "$$\n",
    "\n",
    "with respect to $\\beta_j$, and show that\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\alpha_i}{\\partial \\beta_j} = \\alpha_i (\\delta_{ij} - \\alpha_j).\n",
    "$$\n",
    "\n",
    "**The Kronecker delta** is defined as: $$ \\delta_{ij} = \\begin{cases} 1, & i = j, \\\\ 0, & i \\ne j. \\end{cases} $$\n",
    "\n",
    "*Hint:* Quotient rule."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91fc5cdf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "327e624a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Q6 — Output Derivative w.r.t. Logits\n",
    "\n",
    "Show that\n",
    "\n",
    "$$\n",
    "\\frac{\\partial y}{\\partial \\beta_j} = \\alpha_j (x_j - y).\n",
    "$$\n",
    "\n",
    "*Hint:* Combine linearity of $y$ with Q5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "494e155a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "779ecd34",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Q7 — Gradient\n",
    "\n",
    "Using $\\dfrac{\\partial L}{\\partial y} = y - t$ and Q6, show that\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\beta_j} = (y - t)\\,\\alpha_j\\,(x_j - y).\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b860a9f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e9f125dd",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Q8 — One Update Step\n",
    "\n",
    "With learning rate $\\eta = 0.2$, perform one gradient step:\n",
    "\n",
    "$$\n",
    "\\beta_j' = \\beta_j - \\eta\\,\\frac{\\partial L}{\\partial \\beta_j}.\n",
    "$$\n",
    "\n",
    "At $\\beta_1 = 0,\\; \\beta_2 = 1$, evaluate numerically and state whether attention increased on $x_1$ or $x_2$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417db6f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "36377fb2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Q9 — Finite-Difference Gradient Check (Python)\n",
    "\n",
    "Numerically confirm $\\frac{\\partial L}{\\partial \\beta_j}$ at $(\\beta_1,\\beta_2) = (0,1)$ using a central difference with $\\epsilon = 10^{-5}$:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\beta_j} \\approx \\frac{L(\\beta_j + \\epsilon) - L(\\beta_j - \\epsilon)}{2\\epsilon}\n",
    "$$\n",
    "\n",
    "(holding the other $\\beta$ fixed).  \n",
    "Report the analytic value (from Q7), the finite-difference estimate, and the relative error for $j = 1, 2$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590ed624",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analytic gradients: [-0.1817155  0.1817155]\n",
      "Finite-diff gradients: [-0.1817155  0.1817155]\n",
      "Relative error: [1.13457470e-10 5.61798568e-11]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Given constants\n",
    "x = np.array([2.0, 4.0])\n",
    "t = 3.0\n",
    "beta = np.array([0.0, 1.0])\n",
    "eps = 1e-5\n",
    "\n",
    "\n",
    "analytic_grad = None # TODO\n",
    "fd_gradients = None # TODO\n",
    "\n",
    "\n",
    "print(\"Analytic gradients:\", analytic_grad)\n",
    "print(\"Finite-diff gradients:\", fd_gradients)\n",
    "\n",
    "# Optional: compute relative error\n",
    "rel_error = np.abs((analytic_grad - fd_gradients) / (np.abs(fd_gradients) + 1e-12))\n",
    "print(\"Relative error:\", rel_error)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20a0f63f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Q10 — Visualizing the Loss Surface\n",
    "\n",
    "Compute $L(\\beta_1, \\beta_2)$ on a grid over $[-3, 3]^2$ and plot the 3D surface using `matplotlib`.  \n",
    "Label axes $\\beta_1$, $\\beta_2$, and $L$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e67b9641",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
