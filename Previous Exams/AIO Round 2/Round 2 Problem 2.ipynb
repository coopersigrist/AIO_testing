{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2025 USA-NA-AIO Round 2, Problem 2\n",
        "\n",
        "## Problem 2 (100 points)\n",
        "\n",
        "Multi-head attention (MHA) is a big breakthrough in AI. Based on its original form, there are many variants that improved it.\n",
        "\n",
        "In this problem, you are asked to study multi-head attention and its variants.\n",
        "\n",
        "We use the following notation in this problem.\n",
        "\n",
        "- $B$: batch size. $b$: index of a sample.\n",
        "- $L_1$: length of an attending sequence. $l_1$: index of a position in this sequence.\n",
        "- $L_2$: length of a being attended sequence. $l_2$: index of a position in this sequence.\n",
        "- $D_1$: dimension of a hidden state/token in an attending sequence.\n",
        "- $D_2$: dimension of a hidden state/token in a being attended sequence.\n",
        "- $H$: number of heads. $h$: index of a head.\n",
        "- $D_v$: dimension of a value vector.\n",
        "- $D_{qk}$: dimension of a query/key vector.\n",
        "\n",
        "Before starting this problem, make sure to run the following code first without any change:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run code in this cell\n",
        "\n",
        "\"\"\"\n",
        "DO NOT MAKE ANY CHANGE IN THIS CELL.\n",
        "\"\"\"\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## $\\color{red}{\\text{WARNING !!!}}$\n",
        "\n",
        "Beyond importing libraries/modules/classes/functions in the preceding cell, you are **NOT** allowed to import anything else for the following purposes:\n",
        "\n",
        "- As a part of your final solution. For instance, if a problem asks you to build a model without using sklearn but you use it, then you will not earn points.\n",
        "\n",
        "- Temporarily import something to assist you to get a solution. For instance, if a problem asks you to manually compute eigenvalues but you temporarily use `np.linalg.eig` to get an answer and then delete your code, then you violate the rule.\n",
        "\n",
        "**Rule of thumb:** Each part has its particular purpose to intentionally test you something. Do not attempt to find a shortcut to circumvent the rule.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Part 1 (5 points, non-coding task)\n",
        "\n",
        "Do the following tasks (Reasoning is not required).\n",
        "\n",
        "1. For each hidden state at position $l_1$ in an attending sequence, $x_{l_1} \\in \\mathbb{R}^{D_1}$, we project it into a query vector for head $h$ according to\n",
        "\n",
        "$$q_{l_1, h} = W^Q_h x_{l_1}$$\n",
        "\n",
        "What is the shape of $W^Q_h$?\n",
        "\n",
        "2. For each hidden state at position $l_2$ in a being attended sequence $y_{l_2} \\in \\mathbb{R}^{D_2}$, we project it into a key vector for head $h$ according to\n",
        "\n",
        "$$k_{l_2, h} = W^K_h y_{l_2}$$\n",
        "\n",
        "What is the shape of $W^K_h$?\n",
        "\n",
        "3. For each hidden state at position $l_2$ in a being attended sequence $y_{l_2} \\in \\mathbb{R}^{D_2}$, we project it into a value vector for head $h$ according to\n",
        "\n",
        "$$v_{l_2, h} = W^V_h y_{l_2}$$\n",
        "\n",
        "What is the shape of $W^V_h$?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\n",
        "Answer:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "\n",
        "\"\"\" END OF THIS PART \"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Part 2 (5 points, non-coding task)\n",
        "\n",
        "For $M \\in \\{Q, K, V\\}$, We concatenate $M$-projection matrices $\\{W^M_h : h \\in \\{0, 1, \\cdots, H-1\\}\\}$ along axis 0 as\n",
        "\n",
        "$$W^M = \\begin{bmatrix} W^M_0 \\\\ W^M_1 \\\\ \\vdots \\\\ W^M_{H-1} \\end{bmatrix}$$\n",
        "\n",
        "At each position $l_1$ in an attending sequence, we concatenate queries $\\{q_{l_1, h} : h \\in \\{0, 1, \\cdots, H-1\\}\\}$ along axis 0 to get\n",
        "\n",
        "$$q_{l_1} = \\begin{bmatrix} q_{l_1, 0} \\\\ q_{l_1, 1} \\\\ \\vdots \\\\ q_{l_1, H-1} \\end{bmatrix}$$\n",
        "\n",
        "At each position $l_2$ in a being attended sequence, we concatenate keys/values $m \\in \\{k, v\\}$ $\\{m_{l_2, h} : h \\in \\{0, 1, \\cdots, H-1\\}\\}$ along axis 0 to get\n",
        "\n",
        "$$m_{l_2} = \\begin{bmatrix} m_{l_2, 0} \\\\ m_{l_2, 1} \\\\ \\vdots \\\\ m_{l_2, H-1} \\end{bmatrix}$$\n",
        "\n",
        "Do the following tasks (Reasoning is not required).\n",
        "\n",
        "1. What is the shape of $W^M$ for $M \\in \\{Q, K, V\\}$?\n",
        "2. What is the shape of $q_{l_1}$?\n",
        "3. What is the relationship between $q_{l_1}$ and $W^Q$?\n",
        "4. For $m \\in \\{k, v\\}$, what is the shape of $m_{l_2}$?\n",
        "5. What is the relationship between $m_{l_2}$ and $W^M$?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\n",
        "Answer:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "\n",
        "\"\"\" END OF THIS PART \"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Part 3 (10 points, non-coding task)\n",
        "\n",
        "Define function $\\text{Softmax}: \\mathbb{R}^d \\to \\mathbb{R}^d$, with the $i$th output value as\n",
        "\n",
        "$$\\text{Softmax}_i(z) = \\frac{\\exp(z_i)}{\\sum_{j=0}^{d-1} \\exp(z_j)}$$\n",
        "\n",
        "At position $l_1$ in the attending sequence, its attention score to position $l_2$ in the being attended sequence for head $h$ is denoted as $\\alpha_{h, l_1}^{l_2}$.\n",
        "\n",
        "We can write $\\alpha_{h, l_1}^{l_2}$ in the following form:\n",
        "\n",
        "$$\\alpha_{h, l_1}^{l_2} = \\text{Softmax}_{l_2}\\left( \\boxed{\\color{red}{???}} \\right)$$\n",
        "\n",
        "What is the formula in the above red box (reasoning is not required)?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\n",
        "Answer:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "\n",
        "\"\"\" END OF THIS PART \"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Part 4 (5 points, non-coding task)\n",
        "\n",
        "At position $l_1$ in an attending sequence, for head $h$, the information extracted from attending to a being attended sequence is given by\n",
        "\n",
        "$$o_{h, l_1} = \\sum_{l_2=0}^{L_2-1} \\alpha_{h, l_1}^{l_2} v_{l_2, h}$$\n",
        "\n",
        "We hereafter call $o_{h, l_1}$ a pre-out-projection output vector.\n",
        "\n",
        "Do the following tasks.\n",
        "\n",
        "1. What is the shape of vector $o_{h, l_1}$?\n",
        "\n",
        "2. We concatenate $\\{o_{h, l_1} : h \\in \\{0, 1, \\cdots, H-1\\}\\}$ along axis 0:\n",
        "\n",
        "$$o_{l_1} = \\begin{bmatrix} o_{0, l_1} \\\\ o_{1, l_1} \\\\ \\vdots \\\\ o_{H-1, l_1} \\end{bmatrix}$$\n",
        "\n",
        "What is the shape of $o_{l_1}$?\n",
        "\n",
        "3. We project $o_{l_1}$ to a post-out-projection output vector via an out-projection matrix:\n",
        "\n",
        "$$x^{out}_{l_1} = W^O o_{l_1} \\in \\mathbb{R}^{D_1}$$\n",
        "\n",
        "where\n",
        "\n",
        "$$W^O = \\begin{bmatrix} W^O_0 & W^O_1 & \\cdots & W^O_{H-1} \\end{bmatrix}$$\n",
        "\n",
        "What is the shape of $W^O_h$ for each $h \\in \\{0, 1, \\cdots, H-1\\}$ and $W^O$?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\n",
        "Answer:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "\n",
        "\"\"\" END OF THIS PART \"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Part 5 (10 points, coding task)\n",
        "\n",
        "In this part, you are asked to build your own multi-head attention module that subclasses `nn.Module`.\n",
        "\n",
        "For simplicity, we ignore any masking. That is, each position in an attending sequence attends to all positions in a being attended sequence.\n",
        "\n",
        "In your code, you do not need to worry about whether your code is efficient in an autoprogressive token generation process when your module is used in inference in a GPT-like task.\n",
        "\n",
        "That is, if we use your code in a GPT-like task to autoprogressively generate tokens, it is totally fine if you repeatly generate the same key and value at a given position rather than more efficiently storing their values in cache.\n",
        "\n",
        "1. The class name is `MyMHA`.\n",
        "\n",
        "2. **Attributes:**\n",
        "   - `D_1`: Dimension of a hidden state/token in an attending sequence.\n",
        "   - `D_2`: Dimension of a hidden state/token in a being attended sequence.\n",
        "   - `D_v`: Dimension of a value vector.\n",
        "   - `D_qk`: Dimension of a query/key vector.\n",
        "   - `H`: Number of heads.\n",
        "   - `W_Q`: A linear module whose weights is a query-projection matrix. The shape should be consistant with your answer in Part 2. No bias.\n",
        "   - `W_K`: A linear module whose weights is key-projection matrix. The shape should be consistant with your answer in Part 2. No bias.\n",
        "   - `W_V`: A linear module whose weights is value-projection matrix. The shape should be consistant with your answer in Part 2. No bias.\n",
        "   - `W_O`: A linear module whose weights is an out-projection matrix. The shape should be consistant with your answer in Part 4. No bias.\n",
        "\n",
        "3. **Method `__init__`:**\n",
        "   - Inputs: `D_1`, `D_2`, `D_qk`, `D_v`, `H`\n",
        "   - Outputs: None\n",
        "   - What to do inside this method: Initialize attribute values\n",
        "\n",
        "4. **Method `forward`:**\n",
        "   - Inputs:\n",
        "     - An attending sequence (tensor) with shape `(B, L_1, D_1)`\n",
        "     - A being addended sequence (tensor) with shape `(B, L_2, D_2)`\n",
        "   - Outputs: Post-out-projection outputs with shape `(B, L_1, D_1)`\n",
        "   - What to do inside this method:\n",
        "     - Compute the outputs\n",
        "     - After each operation, add a comment on the tensor shape\n",
        "     - Do not use any loop\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### WRITE YOUR SOLUTION HERE ###\n",
        "\n",
        "\n",
        "\n",
        "\"\"\" END OF THIS PART \"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "Next, let us study a variant of MHA: **Group Query Attention (GQA)**.\n",
        "\n",
        "Recall that in MHA, the number of heads in queries, keys and values are the same, $H$. Thus, query $q_{l_1, h}$ attends to key $k_{l_2, h}$ with the same head index $h$.\n",
        "\n",
        "In GQA, we relax this constraint by allowing keys and values to have $G$ heads ($G \\leq H$), where $G$ is factor of $H$. For instance, if $H = 12$, then $G \\in \\{1, 2, 3, 4, 6, 12\\}$.\n",
        "\n",
        "In GQA, a query $q_{l_1, h}$ with head $h$ is permitted to attend to a key $k_{l_2, g}$ and use value $v_{l_2, g}$ in computing its output with head $g$ if\n",
        "\n",
        "$$h \\equiv g \\pmod{G}$$\n",
        "\n",
        "Thus, each head in keys and values is mapped to $\\frac{H}{G} \\geq 1$ heads in queries.\n",
        "\n",
        "As an example, suppose $H = 12$ and $G = 3$. Then\n",
        "- Head $g = 0$ in keys and values is associated with heads $h = 0, 3, 6, 9$ in queries.\n",
        "- Head $g = 1$ in keys and values is associated with heads $h = 1, 4, 7, 10$ in queries.\n",
        "- Head $g = 2$ in keys and values is associated with heads $h = 2, 5, 8, 11$ in queries.\n",
        "\n",
        "---\n",
        "\n",
        "## Part 6 (5 points, non-coding task)\n",
        "\n",
        "For $M \\in \\{K, V\\}$, Denote the $M$-projection matrix as\n",
        "\n",
        "$$W^{M, GQA} = \\begin{bmatrix} W^{M, GQA}_0 \\\\ \\vdots \\\\ W^{M, GQA}_{G-1} \\end{bmatrix}$$\n",
        "\n",
        "Now, we concatenate $\\frac{H}{G}$ copies of the above matrix along axis 0:\n",
        "\n",
        "$$\\tilde{W}^{M, GQA} = \\begin{bmatrix} W^{M, GQA} \\\\ W^{M, GQA} \\\\ \\vdots \\\\ W^{M, GQA} \\end{bmatrix}$$\n",
        "\n",
        "What is the relationship between $\\text{rank}(\\tilde{W}^{M, GQA})$ and $\\text{rank}(W^{M, GQA})$?\n",
        "\n",
        "Reasoning is required.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\n",
        "Answer:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "\n",
        "\"\"\" END OF THIS PART \"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Part 7 (10 points, coding task)\n",
        "\n",
        "In this part, please build your own GQA module called `MyGQA`.\n",
        "\n",
        "The requirement is pretty much the same as Part 5.\n",
        "\n",
        "- Do NOT create $\\frac{H}{G}$ copies of key-projection and value-projection matrices. Otherwise, you will use too much unnecessary memory.\n",
        "- No loop is allowed.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### WRITE YOUR SOLUTION HERE ###\n",
        "\n",
        "\n",
        "\n",
        "\"\"\" END OF THIS PART \"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Part 8 (5 points, non-coding task)\n",
        "\n",
        "MHA is a special case of GQA. Explain why.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\n",
        "Answer:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "\n",
        "\"\"\" END OF THIS PART \"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "Now, let us study another variant of MHA: **Multi-head Latent Attention (MLA)**. MLA was introduced by DeepSeek. It is a core component of DeepSeek's large language model (LLM).\n",
        "\n",
        "The key intuition of MLA is as follows. In MHA, the key and value projection matrices\n",
        "\n",
        "$$W^{K, MHA} \\in \\mathbb{R}^{H \\cdot D_{qk} \\times D_2}, \\quad W^{V, MHA} \\in \\mathbb{R}^{H \\cdot D_v \\times D_2}$$\n",
        "\n",
        "may be high dimensional.\n",
        "\n",
        "For instance, suppose $H \\cdot D_{qk} = H \\cdot D_v = D_2 = 4096$.\n",
        "\n",
        "However, it is not necessarily the case that these matrices are with high ranks (such as 4096). Their actual ranks (or top few ranks that make their truncated singular value decomposition (SVD) to be close to the actual matrices) may be much lower than that.\n",
        "\n",
        "To capture the low-rank feature, MLA proposed the following model:\n",
        "\n",
        "$$W^{K, MHA} = W^{UK, MLA} W^{DKV, MLA}$$\n",
        "\n",
        "$$W^{V, MHA} = W^{UV, MLA} W^{DKV, MLA}$$\n",
        "\n",
        "where\n",
        "\n",
        "- $W^{DKV, MLA} \\in \\mathbb{R}^{r \\times D_2}$: down-projection matrix for computing keys and values.\n",
        "- $W^{UK, MLA} \\in \\mathbb{R}^{H \\cdot D_{qk} \\times r}$: up-projection matrix for computing keys.\n",
        "- $W^{UV, MLA} \\in \\mathbb{R}^{H \\cdot D_v \\times r}$: up-projection matrix for computing values.\n",
        "\n",
        "In practice, rank $r$ is typically much smaller than $\\min\\{H \\cdot D_{qk}, H \\cdot D_v, D_2\\}$.\n",
        "\n",
        "---\n",
        "\n",
        "In all remaining parts of this problem, to simplify your analysis and highlight the relationships of MHA, GQA and MLA, we make the following assumptions:\n",
        "\n",
        "- $D_1 = D_2 = D$.\n",
        "- $D_{qk} = D_v = d$.\n",
        "- $d$ is a factor of $D$.\n",
        "\n",
        "Under these assumptions, the number heads $H$ satisfies\n",
        "\n",
        "$$H = \\frac{D}{d}$$\n",
        "\n",
        "---\n",
        "\n",
        "## Part 9 (10 points, non-coding task)\n",
        "\n",
        "In this part, you are asked to prove that GQA can be equivalently represented by MLA.\n",
        "\n",
        "In your solution, it is sufficient for you to prove that for $M \\in \\{K, V\\}$, for matrix\n",
        "\n",
        "$$\\tilde{W}^{M, GQA} = \\begin{bmatrix} W^{M, GQA} \\\\ W^{M, GQA} \\\\ \\vdots \\\\ W^{M, GQA} \\end{bmatrix} \\in \\mathbb{R}^{D \\times D}$$\n",
        "\n",
        "(defined in Part 6) who is the concatenation of $\\frac{H}{G}$ copies of\n",
        "\n",
        "$$W^{M, GQA} = \\begin{bmatrix} W^{M, GQA}_0 \\\\ \\vdots \\\\ W^{M, GQA}_{G-1} \\end{bmatrix} \\in \\mathbb{R}^{G \\cdot d \\times D}$$\n",
        "\n",
        "matrix $\\tilde{W}^{M, GQA}$ can be decomposed as\n",
        "\n",
        "$$\\tilde{W}^{M, GQA} = W^{UM, MLA} W^{DKV, MLA}$$\n",
        "\n",
        "where\n",
        "\n",
        "- $W^{DKV, MLA} \\in \\mathbb{R}^{r \\times D}$: down-projection matrix for computing keys and values.\n",
        "- $W^{UM, MLA} \\in \\mathbb{R}^{D \\times r}$: up-projection matrix for computing $M$ (keys or values).\n",
        "- $r = G \\cdot d$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\n",
        "Answer:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "\n",
        "\"\"\" END OF THIS PART \"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Part 10 (5 points, coding task)\n",
        "\n",
        "This question follows Part 9.\n",
        "\n",
        "You are asked to define a function called `GQA_2_MLA` that performs the following tasks:\n",
        "\n",
        "**Input:**\n",
        "\n",
        "- `W_M_GQA`: A numpy array with shape `(r, D)`, where `r` is guaranteed to be a factor of `D` (not something you need to worry about).\n",
        "\n",
        "**Outputs:**\n",
        "\n",
        "- `W_DKV_MLA`: A numpy array with shape `(r, D)`.\n",
        "- `W_UM_MLA`: A numpy array with shape `(D, r)`.\n",
        "\n",
        "**Things to do inside this function:**\n",
        "\n",
        "- Compute `W_M_GQA_tilde` that concatenates `D/r` copies of `W_M_GQA` along axis 0.\n",
        "- Print the shapes of `W_UM_MLA` and `W_DKV_MLA`.\n",
        "- Print the mean-squared error between `W_M_GQA_tilde` and `W_UM_MLA @ W_DKV_MLA`.\n",
        "\n",
        "**Hints:**\n",
        "\n",
        "- You may use `np.linalg`.\n",
        "- PyTorch is not allowed.\n",
        "- No loop in your code.\n",
        "\n",
        "After defining this function, test it with the input `np.random.randn(4, 24)`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### WRITE YOUR SOLUTION HERE ###\n",
        "\n",
        "\n",
        "\n",
        "\"\"\" END OF THIS PART \"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Part 11 (10 points, non-coding task)\n",
        "\n",
        "So far, we have proved that GQA can always be represented by MLA.\n",
        "\n",
        "In this part, you are asked to prove that GQA is not equivalent to MLA. What you need to do is to find one example that MLA cannot be represented as GQA.\n",
        "\n",
        "To be specific, please do the following things:\n",
        "\n",
        "1. Construct $W^{DKV, MLA} \\in \\mathbb{R}^{1 \\times 2}$.\n",
        "\n",
        "2. Construct $W^{UM, MLA} \\in \\mathbb{R}^{2 \\times 1}$.\n",
        "\n",
        "3. Do matrix multiplication $W^{UM, MLA} W^{DKV, MLA}$.\n",
        "\n",
        "4. Show that this product matrix is not the concatenation of two copies of 1-by-2 matrices along axis 0.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\n",
        "Answer:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "\n",
        "\"\"\" END OF THIS PART \"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "MLA does not only enjoy its advantage of being more general than MHA and GQA, it is also computationally more efficient.\n",
        "\n",
        "**An intuitive approach of computing MLA.**\n",
        "\n",
        "1. Compute the key-projection matrix $W^{UK, MLA} W^{DKV, MLA} \\in \\mathbb{R}^{D \\times D}$ and the value-projection matrix $W^{UV, MLA} W^{DKV, MLA} \\in \\mathbb{R}^{D \\times D}$.\n",
        "2. Follow the standard steps in MHA.\n",
        "\n",
        "This approach is hereafter called a **vanilla approach**. This approach fails to enjoy the low-rank feature of $W^{DKV, MLA}$, $W^{UK, MLA}$, and $W^{UV, MLA}$.\n",
        "\n",
        "---\n",
        "\n",
        "## Part 12 (10 points, non-coding task)\n",
        "\n",
        "In this part, you are asked to study an alternative approach to compute MLA.\n",
        "\n",
        "1. Find a head-independent reduced key-projection matrix $\\hat{W}^{K, MLA} \\in \\mathbb{R}^{r \\times D}$ and a reduced query-projection matrix $\\hat{W}^{Q, MLA} \\in \\mathbb{R}^{H \\cdot r \\times D}$, such that\n",
        "\n",
        "   - The reduced key at position $l_2$ for head $h$ in a being attended sequence is head-independent and is given by:\n",
        "\n",
        "     $$\\hat{k}_{l_2} = \\hat{W}^{K, MLA} y_{l_2} \\in \\mathbb{R}^r$$\n",
        "\n",
        "   - The reduced query at position $l_1$ for head $h$ in an attending sequence is given by:\n",
        "\n",
        "     $$\\hat{q}_{l_1, h} = \\hat{W}^{Q, MLA}_h x_{l_1} \\in \\mathbb{R}^r$$\n",
        "\n",
        "     where\n",
        "\n",
        "     $$\\hat{W}^{Q, MLA} = \\begin{bmatrix} \\hat{W}^{Q, MLA}_0 \\\\ \\hat{W}^{Q, MLA}_1 \\\\ \\vdots \\\\ \\hat{W}^{Q, MLA}_{H-1} \\end{bmatrix}$$\n",
        "\n",
        "   - The attention score (query-key similarity) is invariant in both the original and the reduced forms. That is\n",
        "\n",
        "     $$\\frac{q_{l_1, h}^\\top k_{l_2, h}}{\\sqrt{D/H}} = \\frac{\\hat{q}_{l_1, h}^\\top \\hat{k}_{l_2}}{\\sqrt{r}} \\tag{1}$$\n",
        "\n",
        "2. Find a head-independent reduced value-projection matrix $\\hat{W}^{V, MLA} \\in \\mathbb{R}^{r \\times D}$ and a reduced out-projection matrix $\\hat{W}^{O, MLA} \\in \\mathbb{R}^{D \\times H \\cdot r}$, such that\n",
        "\n",
        "   - The reduced value with head $h$ on position $l_2$ in a being attended sequence is head-independent and is given by:\n",
        "\n",
        "     $$\\hat{v}_{l_2} = \\hat{W}^{V, MLA} y_{l_2} \\in \\mathbb{R}^r$$\n",
        "\n",
        "   - Post-out-projection is invariant in both the original and the reduced forms.\n",
        "\n",
        "     Let\n",
        "\n",
        "     $$\\hat{W}^{O, MLA} = \\begin{bmatrix} \\hat{W}^{O, MLA}_0 & \\hat{W}^{O, MLA}_1 & \\cdots & \\hat{W}^{O, MLA}_{H-1} \\end{bmatrix}$$\n",
        "\n",
        "     Then we must have\n",
        "\n",
        "     $$\\sum_{h=0}^{H-1} W^O_h \\sum_{l_2=0}^{L_2-1} \\alpha_{h, l_1}^{l_2} v_{l_2, h} = \\sum_{h=0}^{H-1} \\hat{W}^{O, MLA}_h \\sum_{l_2=0}^{L_2-1} \\alpha_{h, l_1}^{l_2} \\hat{v}_{l_2} \\tag{2}$$\n",
        "\n",
        "Your answer of $\\hat{W}^{K, MLA}$, $\\hat{W}^{V, MLA}$, $\\hat{W}^{Q, MLA}$, and $\\hat{W}^{O, MLA}$ should be written in terms of $W^{DKV}$, $W^{UK}$, $W^{UV}$, $W^Q$, and $W^O$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\n",
        "Answer:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "\n",
        "\"\"\" END OF THIS PART \"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Part 13 (5 points, coding task)\n",
        "\n",
        "Do the following tasks:\n",
        "\n",
        "1. Define a function called `reduced_matrices`.\n",
        "   - **Input arguments:** `W_DKV`, `W_UK`, `W_UV`, `W_Q`, `W_O`, `H`\n",
        "   - **Outputs:** `W_K_MLA_hat`, `W_V_MLA_hat`, `W_Q_MLA_hat`, `W_O_MLA_hat`\n",
        "   - **Requirement of your code:**\n",
        "     - The code of computing each output must be in one line\n",
        "     - Loop is not allowed\n",
        "\n",
        "2. Set your device as gpu:\n",
        "   ```python\n",
        "   device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "   ```\n",
        "\n",
        "3. Construct the following synthetic data:\n",
        "   ```python\n",
        "   D = 1024\n",
        "   H = 32\n",
        "   D_qkv = D // H\n",
        "   r = 50\n",
        "   \n",
        "   W_DKV = torch.randn(r, D)\n",
        "   W_UK = torch.randn(D, r)\n",
        "   W_UV = torch.randn(D, r)\n",
        "   W_Q = torch.randn(D, D)\n",
        "   W_O = torch.randn(D, D)\n",
        "   \n",
        "   B = 32\n",
        "   L_1 = 100\n",
        "   L_2 = 300\n",
        "   \n",
        "   x = torch.randn(B, L_1, D).to(device)\n",
        "   y = torch.randn(B, L_2, D).to(device)\n",
        "   ```\n",
        "\n",
        "4. Study a vanilla attention model\n",
        "   - Initialize the model\n",
        "     ```python\n",
        "     model_MHA_vanilla = MyMHA(D, D, D_qkv, D_qkv, H)\n",
        "     ```\n",
        "   - Update model parameters\n",
        "     - `model_MHA_vanilla.W_K.weight`, `model_MHA_vanilla.W_V.weight`, `model_MHA_vanilla.W_Q.weight`, `model_MHA_vanilla.W_O.weight`\n",
        "   - Compute the output\n",
        "     ```python\n",
        "     output_vanilla = model_MHA_vanilla(x, y)\n",
        "     ```\n",
        "\n",
        "5. Study a reduced attention model\n",
        "   - Initialize the model\n",
        "     ```python\n",
        "     model_MHA_reduced = MyMHA(D, D, r, r, H)\n",
        "     ```\n",
        "   - Update model parameters\n",
        "     - `model_MHA_reduced.W_K.weight`, `model_MHA_reduced.W_V.weight`, `model_MHA_reduced.W_Q.weight`, `model_MHA_reduced.W_O.weight`\n",
        "   - Compute the output\n",
        "     ```python\n",
        "     output_reduced = model_MHA_reduced(x, y)\n",
        "     ```\n",
        "\n",
        "6. Check the correctness of the reduced model by computing and printing a relative error:\n",
        "   ```python\n",
        "   relative_error = mse_output**.5 / torch.mean(output_vanilla**2)**.5\n",
        "   ```\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### WRITE YOUR SOLUTION HERE ###\n",
        "\n",
        "\n",
        "\n",
        "\"\"\" END OF THIS PART \"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Part 14 (5 points, non-coding task)\n",
        "\n",
        "In generative AI, such as GPT, we autoprogressively generate tokens. For a given position $l$, the keys and values on this position $k_l$ and $v_l$ are repeatly used in generating tokens for positions $l' > l$.\n",
        "\n",
        "Therefore, the values of $k_l$ and $v_l$ are typically stored in cache (no need to revise your code in earlier parts if your code does not support this). We call such storage as **kv-cache**.\n",
        "\n",
        "Do the following tasks to compute kv-cache in different models while doing autoregressive inference: (reasoning is required)\n",
        "\n",
        "1. In MHA, the kv-cache at each position is $2D$. Explain why.\n",
        "\n",
        "2. In MLA, what is the kv-cache at each position?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "```\n",
        "Answer:\n",
        "\n",
        "\n",
        "\n",
        "```\n",
        "\n",
        "\"\"\" END OF THIS PART \"\"\"\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
