{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2025 USA-NA-AIO Round 2, Problem 3 — ANSWERS\n",
        "\n",
        "## Problem 3 (100 points)\n",
        "\n",
        "In this problem, you are asked to study Contrastive Language-Image Pre-Training (CLIP), a powerful tool in multimodal AI."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run code in this cell\n",
        "\n",
        "\"\"\"\n",
        "DO NOT MAKE ANY CHANGE IN THIS CELL.\n",
        "HINT: If something is not corrected installed, simply run this cell for few more times.\n",
        "\"\"\"\n",
        "!pip install datasets transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## $\\color{red}{\\text{WARNING !!!}}$\n",
        "\n",
        "Beyond importing libraries/modules/classes/functions in the following cell, you are **NOT** allowed to import anything else for the following purposes:\n",
        "\n",
        "- As a part of your final solution. For instance, if a problem asks you to build a model without using sklearn but you use it, then you will not earn points.\n",
        "\n",
        "- Temporarily import something to assist you to get a solution. For instance, if a problem asks you to manually compute eigenvalues but you temporarily use `np.linalg.eig` to get an answer and then delete your code, then you violate the rule.\n",
        "\n",
        "**Rule of thumb:** Each part has its particular purpose to intentionally test you something. Do not attempt to find a shortcut to circumvent the rule.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run code in this cell\n",
        "\n",
        "\"\"\"\n",
        "DO NOT MAKE ANY CHANGE IN THIS CELL.\n",
        "\"\"\"\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "from transformers import BertTokenizer, BertModel, ViTModel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "We will use flickr30k dataset to do image-language matching.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Run code in this cell\n",
        "\n",
        "\"\"\"\n",
        "DO NOT MAKE ANY CHANGE IN THIS CELL.\n",
        "\"\"\"\n",
        "from datasets import load_dataset\n",
        "dataset_train = load_dataset(\"USAAIO/2025-Round2-Problem3\", split='train')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Part 1 (5 points, coding task)\n",
        "\n",
        "Do the following tasks to explore the properties of `dataset_train`:\n",
        "\n",
        "1. `dataset_train` is a list-like object. Print the number of elements in it.\n",
        "\n",
        "2. Consider index `idx = 2025`. Print the type of `dataset_train[idx]`.\n",
        "\n",
        "3. Print all keys in `dataset_train[idx]`.\n",
        "\n",
        "4. Name the value associated with the key `image` as `image_PIL`. Print it.\n",
        "\n",
        "5. Convert `image_PIL` to a NumPy array object, called `image_np`. Print `image_np` and its shape.\n",
        "\n",
        "6. Display this image by using `plt.imshow`.\n",
        "\n",
        "7. Print the value associated with the key `alt_text`. Print its type."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### WRITE YOUR SOLUTION HERE ###\n",
        "\n",
        "print(len(dataset_train))\n",
        "\n",
        "idx = 2025\n",
        "print(type(dataset_train[idx]))\n",
        "print(dataset_train[idx].keys())\n",
        "\n",
        "image_PIL = dataset_train[idx]['image']\n",
        "print(image_PIL)\n",
        "\n",
        "image_np = np.array(image_PIL)\n",
        "print(image_np)\n",
        "print(image_np.shape)\n",
        "\n",
        "plt.imshow(image_np)\n",
        "plt.show()\n",
        "\n",
        "print(dataset_train[idx]['alt_text'])\n",
        "print(type(dataset_train[idx]['alt_text']))\n",
        "\n",
        "\"\"\" END OF THIS PART \"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Part 2 (5 points, coding task)\n",
        "\n",
        "This dataset is too big. In our contest, we only use a small portion with 1000 samples.\n",
        "\n",
        "To avoid introducing any bias, we will randomly select 1000 distinct samples.\n",
        "\n",
        "Use NumPy to randomly select 1000 sample indices.\n",
        "- Use the random seed number `2025` to generated randomized indices. After the generation is completed, reset the seed number back to `None`.\n",
        "- The name of the output is called `indices`. It must be a list that contains 1000 integer type (not numpy array integers) objects."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### WRITE YOUR SOLUTION HERE ###\n",
        "\n",
        "np.random.seed(2025)\n",
        "indices = np.random.permutation(len(dataset_train))[:1000]\n",
        "np.random.seed()\n",
        "indices = [int(idx) for idx in indices]\n",
        "\n",
        "\"\"\" END OF THIS PART \"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Part 3 (5 points, coding task)\n",
        "\n",
        "In this part, we create our image and text datasets.\n",
        "\n",
        "- All sample indices are selected from `indices` generated in Part 2.\n",
        "- All images (resp. texts) are extracted from the key `image` (resp. `alt_text`).\n",
        "- The image (resp. text) dataset is called `image_list` (resp. `text_list`). The data type of both datasets are `list`.\n",
        "- In `image_list`, each element is a PIL object.\n",
        "- In `text_list`, each element is a string object."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### WRITE YOUR SOLUTION HERE ###\n",
        "\n",
        "image_list = [dataset_train[idx]['image'] for idx in indices]\n",
        "text_list = [dataset_train[idx]['alt_text'][0] for idx in indices]\n",
        "\n",
        "\"\"\" END OF THIS PART \"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Part 4 (5 points, coding task)\n",
        "\n",
        "In this part, we preprocess image data.\n",
        "\n",
        "1. Your job is to create a tensor `images_pt` from `image_list` that has shape `(1000, 3, 224, 224)` and datatype `float64`.\n",
        "\n",
        "2. The data range is from -1 to 1.\n",
        "\n",
        "3. **Hint:** If `a` is a PIL object, then you can use `a.resize` to resize it.\n",
        "\n",
        "4. Print `images_pt.shape`.\n",
        "\n",
        "5. Print `images_pt.dtype`.\n",
        "\n",
        "6. Print `images_pt[5]`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### WRITE YOUR SOLUTION HERE ###\n",
        "\n",
        "images_pt_list = []\n",
        "\n",
        "for image in image_list:\n",
        "    image = image.resize((224,224)) # Resize the image\n",
        "    image_np = np.array(image) # Convert to numpy array\n",
        "    image_pt = torch.from_numpy(image_np) # Convert to pytorch tensor\n",
        "    image_pt = image_pt.permute(2,0,1) # Permute the dimension\n",
        "    image_pt = image_pt / 255 # Normalize value between 0 and 1\n",
        "    image_pt = image_pt * 2 - 1 # Normalize value between -1 and 1\n",
        "    images_pt_list.append(image_pt)\n",
        "\n",
        "images_pt = torch.stack(images_pt_list)\n",
        "\n",
        "print(images_pt.shape)\n",
        "print(images_pt.dtype)\n",
        "print(images_pt[5])\n",
        "\n",
        "\"\"\" END OF THIS PART \"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Part 5 (5 points, non-coding task)\n",
        "\n",
        "Note that our final goal is to build a CLIP neural network. For the image data, we will use Vision Transformers (ViT) to extract image embeddings.\n",
        "\n",
        "With the above high level information, please explain the reasons behind the following things that you did in Part 4.\n",
        "\n",
        "1. Why the channel dimension is ahead of the height and width dimensions?\n",
        "\n",
        "2. Why the sizes of all images are normalized to (224, 224)?\n",
        "\n",
        "3. Why each pixel value is normalized between -1 and 1?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Answer:**\n",
        "\n",
        "1. The input of ViT requires the channel dimension to go ahead of the height and width dimensions.\n",
        "\n",
        "2. ViT model requires this dimension.\n",
        "\n",
        "3. ViT model requires data to fall into this range.\n",
        "\n",
        "\"\"\" END OF THIS PART \"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### WRITE YOUR SOLUTION HERE ###\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "token_id_list = tokenizer(text_list)['input_ids']\n",
        "\n",
        "print(token_id_list)\n",
        "print(type(token_id_list))\n",
        "print(len(token_id_list))\n",
        "\n",
        "print(token_id_list[5])\n",
        "print(type(token_id_list[5]))\n",
        "print(type(token_id_list[5][0]))\n",
        "\n",
        "token_id_list = [torch.tensor(token_id_list[idx]) for idx in range(len(token_id_list))]\n",
        "print(token_id_list[5:7])\n",
        "print(token_id_list[5][0].dtype)\n",
        "\n",
        "\"\"\" END OF THIS PART \"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Part 7 (5 points, non-coding task)\n",
        "\n",
        "This part follows Part 6.\n",
        "\n",
        "Do the following tasks.\n",
        "\n",
        "1. Explain why token lists of all samples begin with token ID 101.\n",
        "2. Explain why token lists of all samples end with token ID 102.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### WRITE YOUR SOLUTION HERE ###\n",
        "\n",
        "class MyDataset(Dataset):\n",
        "    def __init__(self, images_pt, token_id_list):\n",
        "        self.images_pt = images_pt\n",
        "        self.token_id_list = token_id_list\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.token_id_list)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.images_pt[idx], self.token_id_list[idx]\n",
        "\n",
        "CLIP_dateset = MyDataset(images_pt, token_id_list)\n",
        "\n",
        "\"\"\" END OF THIS PART \"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Part 9 (5 points, coding task)\n",
        "\n",
        "### Part 9.1\n",
        "\n",
        "Define your own collate function.\n",
        "\n",
        "The function name is `my_collate_fn`.\n",
        "\n",
        "**Padding**\n",
        "\n",
        "For text data, let the longest sample be with K tokens.\n",
        "\n",
        "Consider another text sample with L tokens satisfying L < K. Then, in addition to those L tokens, this sample is padded with K-L padding tokens whose values are 0.\n",
        "\n",
        "**Outputs**\n",
        "\n",
        "- `token_id_batch`. If the batch size is B and the longest sample in the text data has K tokens, then `token_id_batch` is a tensor with shape (B,K).\n",
        "\n",
        "- `attention_mask_batch`. This is a tensor that has shape (B,K). If a position is occupied by a non-padding token, its value is 1. Otherwise, if it is occupied by a padding token, its value is 0. Data types are int64.\n",
        "\n",
        "- `image_batch`. This is a tensor that has shape (B,3,224,224).\n",
        "\n",
        "### Part 9.2\n",
        "\n",
        "Define a DataLoader object called `CLIP_dataloader`.\n",
        "\n",
        "- Set `batch_size = 16`.\n",
        "\n",
        "- Set `shuffle = True`.\n",
        "\n",
        "- Use the collate function defined in Part 9.1.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### WRITE YOUR SOLUTION HERE ###\n",
        "\n",
        "# Part 9.1\n",
        "\n",
        "def my_collate_fn(batch):\n",
        "    image_batch_input, token_id_batch_input = zip(*batch)\n",
        "\n",
        "    image_batch = torch.stack(image_batch_input)\n",
        "\n",
        "    max_len_token_id = max([len(token_id) for token_id in token_id_batch_input])\n",
        "    token_id_batch = []\n",
        "    attention_mask_batch = []\n",
        "\n",
        "    for token_id in token_id_batch_input:\n",
        "        token_id_batch.append(torch.concatenate([token_id, torch.zeros(max_len_token_id - len(token_id), dtype=torch.int64)]))\n",
        "        attention_mask_batch.append(torch.concatenate([torch.ones(len(token_id), dtype=torch.int64), \\\n",
        "                                                       torch.zeros(max_len_token_id - len(token_id), dtype=torch.int64)]))\n",
        "\n",
        "    token_id_batch = torch.stack(token_id_batch)\n",
        "    attention_mask_batch = torch.stack(attention_mask_batch)\n",
        "\n",
        "    return image_batch, token_id_batch, attention_mask_batch\n",
        "\n",
        "# Part 9.2\n",
        "\n",
        "batch_size = 16\n",
        "\n",
        "CLIP_dataloader = DataLoader(CLIP_dateset, batch_size=batch_size, shuffle=True, collate_fn=my_collate_fn)\n",
        "\n",
        "\"\"\" END OF THIS PART \"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Part 10 (5 points, non-coding task)\n",
        "\n",
        "In this part, you are asked to answer some questions about a CLIP model that you shall build in the next part.\n",
        "\n",
        "Write your answers in the text cell below.\n",
        "\n",
        "To get answers, you may need to run experimental code to better learn the ViT and Bert models.\n",
        "\n",
        "We only grade your answers in the text cell.\n",
        "\n",
        "\n",
        "**1. Image encoder**\n",
        "\n",
        "- Define `model_image = ViTModel.from_pretrained('google/vit-base-patch16-224')`. We use all blocks except the last pooler layer. That is, this ViT model has two outputs: with their key names as `last_hidden_state` and `pooler_output`. You should take the value associated with the key `last_hidden_state`.\n",
        "\n",
        "- From the last hidden state, we project from position 0 to a latent space with dimension `embedding_size` (e.g., 512). The output is called **image embedding**.\n",
        "\n",
        "\n",
        "**2. Text encoder**\n",
        "\n",
        "- Define `model_text = BertModel.from_pretrained('bert-base-uncased')`. We use all blocks except the last pooler layer. That is, this Bert model has two outputs: with their key names as `last_hidden_state` and `pooler_output`. You should take the value associated with the key `last_hidden_state`.\n",
        "\n",
        "- From the last hidden state, we project from position 0 to a latent space with dimension `embedding_size` (e.g., 512). The output is called **text embedding**.\n",
        "\n",
        "**Answer the following questions.** (Reasoning is required only for Question 3)\n",
        "\n",
        "1. Let `image_batch` be with shape `(B,3,224,224)`. What is the shape of `model_image(image_batch)['last_hidden_state']`?\n",
        "\n",
        "2. Let `token_id_batch` and `attention_mask_batch` be with shape `(B,L)`. What is the shape of `model_text(input_ids = token_id_batch, attention_mask = attention_mask_batch)['last_hidden_state']`?\n",
        "\n",
        "3. For both the image encoder and the text encoder, we project the last hidden state from position 0 to a latent space with the same dimension `embedding_size`.\n",
        "\n",
        "   3.1. Why do we add this additional out-projection layer?\n",
        "\n",
        "   3.2. Why this layer is added on position 0 only?\n",
        "\n",
        "   3.3. Why the output dimensions from these two encoders are the same?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### DO YOUR EXPERIMENTAL STUDY HERE ###\n",
        "\n",
        "image_batch, token_id_batch, attention_mask_batch = next(iter(CLIP_dataloader))\n",
        "\n",
        "model_image = ViTModel.from_pretrained('google/vit-base-patch16-224')\n",
        "\n",
        "print(model_image(image_batch).keys())\n",
        "\n",
        "print(model_image(image_batch)['last_hidden_state'].shape)\n",
        "\n",
        "model_text = BertModel.from_pretrained('bert-base-uncased')\n",
        "\n",
        "print(model_text(input_ids=token_id_batch, attention_mask=attention_mask_batch).keys())\n",
        "\n",
        "print(model_text(input_ids=token_id_batch, attention_mask=attention_mask_batch)['last_hidden_state'].shape)\n",
        "\n",
        "\"\"\" END OF THIS PART \"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Answer:**\n",
        "\n",
        "1. The shape of `model_image(image_batch)['last_hidden_state']` is **(B, 197, 768)**.\n",
        "   - 197 = 1 (CLS token) + 196 (14×14 patches from 224×224 image with patch size 16)\n",
        "   - 768 is the hidden dimension of ViT-base\n",
        "\n",
        "2. The shape of `model_text(input_ids=token_id_batch, attention_mask=attention_mask_batch)['last_hidden_state']` is **(B, L, 768)**.\n",
        "   - L is the sequence length (number of tokens)\n",
        "   - 768 is the hidden dimension of BERT-base\n",
        "\n",
        "3.1. We add this additional out-projection layer to map the hidden representations from both encoders to a common latent space where image and text embeddings can be compared directly using similarity metrics (e.g., cosine similarity).\n",
        "\n",
        "3.2. Position 0 is used because:\n",
        "   - For ViT, position 0 corresponds to the [CLS] token which aggregates information from all image patches through self-attention.\n",
        "   - For BERT, position 0 also corresponds to the [CLS] token which serves as the sentence-level representation that captures the meaning of the entire text.\n",
        "\n",
        "3.3. The output dimensions must be the same so that we can compute similarity (e.g., dot product or cosine similarity) between image embeddings and text embeddings in the shared latent space. This is essential for CLIP's contrastive learning objective.\n",
        "\n",
        "\"\"\" END OF THIS PART \"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Part 11 (5 points, coding task)\n",
        "\n",
        "In this part, you are asked to build your CLIP model.\n",
        "\n",
        "- The class name is `MyCLIP`. It subclasses `nn.Module`.\n",
        "\n",
        "- **`__init__`:**\n",
        "\n",
        "    - It takes one input argument - the size of the final embedding of text and image data. Set its default value as 512.\n",
        "\n",
        "    - Attribute `log_tau` is the log of temperature. It is a learnable parameter. Its initial value follows the standard normal distribution.\n",
        "\n",
        "- **`__forward__`:**\n",
        "\n",
        "    - It returns two objects: image embedding, text embedding."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### WRITE YOUR SOLUTION HERE ###\n",
        "\n",
        "class MyCLIP(nn.Module):\n",
        "    def __init__(self, embedding_size=512):\n",
        "        super().__init__()\n",
        "        self.model_image = ViTModel.from_pretrained('google/vit-base-patch16-224')\n",
        "        self.model_text = BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.embedding_size = embedding_size\n",
        "        self.last_layer_image = nn.Linear(768, self.embedding_size)\n",
        "        self.last_layer_text = nn.Linear(768, self.embedding_size)\n",
        "        self.log_tau = nn.Parameter(torch.randn(1))\n",
        "\n",
        "    def encoder_image(self, image_batch):\n",
        "        image_embedding = self.model_image(image_batch)['last_hidden_state'][:,0]\n",
        "        image_embedding = self.last_layer_image(image_embedding)\n",
        "        return image_embedding\n",
        "\n",
        "    def encoder_text(self, token_id_batch, attention_mask_batch):\n",
        "        text_embedding = self.model_text(input_ids=token_id_batch, attention_mask=attention_mask_batch)['last_hidden_state'][:,0]\n",
        "        text_embedding = self.last_layer_text(text_embedding)\n",
        "        return text_embedding\n",
        "\n",
        "    def forward(self, image_batch, token_id_batch, attention_mask_batch):\n",
        "        return self.encoder_image(image_batch), self.encoder_text(token_id_batch, attention_mask_batch)\n",
        "\n",
        "\"\"\" END OF THIS PART \"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Part 12 (5 points, non-coding task)\n",
        "\n",
        "Explain why we use $\\log \\tau$ as an attribute, not $\\tau$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Answer:**\n",
        "\n",
        "$\\log \\tau$ can take all real values. So we do not have to worry about its range.\n",
        "\n",
        "However, $\\tau$ must be positive. Hence, if we use $\\tau$, we always need to take care of its domain.\n",
        "\n",
        "\"\"\" END OF THIS PART \"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Part 13 (5 points, coding task)\n",
        "\n",
        "Do the following tasks:\n",
        "\n",
        "1. Define your model by calling `model_CLIP = MyCLIP()`.\n",
        "\n",
        "2. Fix all parameter values in the ViT and Bert blocks in your model. That is, you are only allowed to train:\n",
        "   - Out-projection matrices in the image and text encoders.\n",
        "   - Temperature.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### WRITE YOUR SOLUTION HERE ###\n",
        "\n",
        "model_CLIP = MyCLIP()\n",
        "\n",
        "for param in model_CLIP.model_text.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "for param in model_CLIP.model_image.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "\"\"\" END OF THIS PART \"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Part 14 (5 points, coding task)\n",
        "\n",
        "Do the following tasks:\n",
        "\n",
        "1. Set the learning rate as `1e-3`.\n",
        "\n",
        "2. Choose your optimization algorithm as Adam.\n",
        "\n",
        "3. Define an optimizer.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### WRITE YOUR SOLUTION HERE ###\n",
        "\n",
        "lr = 1e-3\n",
        "optimizer = optim.Adam(model_CLIP.parameters(), lr=lr)\n",
        "\n",
        "\"\"\" END OF THIS PART \"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Part 15 (5 points, coding task)\n",
        "\n",
        "In this part, you are asked to define a loss function.\n",
        "\n",
        "Let $I_i$ and $T_j$ be image $i$'s embedding and text $j$'s embedding, respectively. Let $B$ be the batch size. Let $\\tau$ be the temperature.\n",
        "\n",
        "Then the loss function is defined as\n",
        "\n",
        "$$\\mathcal{L} = \\frac{1}{2} \\left( -\\frac{1}{B} \\sum_{i=0}^{B-1} \\log \\frac{\\exp(\\text{SIM}(I_i, T_i) / \\tau)}{\\sum_{j=0}^{B-1} \\exp(\\text{SIM}(I_i, T_j) / \\tau)} - \\frac{1}{B} \\sum_{i=0}^{B-1} \\log \\frac{\\exp(\\text{SIM}(I_i, T_i) / \\tau)}{\\sum_{j=0}^{B-1} \\exp(\\text{SIM}(I_j, T_i) / \\tau)} \\right),$$\n",
        "\n",
        "where\n",
        "\n",
        "$$\\text{SIM}(I_i, T_j) = \\frac{I_i^\\top T_j}{\\|I_i\\|_2 \\|T_j\\|_2}.$$\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### WRITE YOUR SOLUTION HERE ###\n",
        "\n",
        "def CLIP_loss_fn(image_embedding, text_embedding):\n",
        "    image_embedding = image_embedding / torch.norm(image_embedding, dim=-1, keepdim=True)\n",
        "    text_embedding = text_embedding / torch.norm(text_embedding, dim=-1, keepdim=True)\n",
        "    sim = torch.sum(image_embedding.unsqueeze(1) * text_embedding.unsqueeze(0), dim=-1)\n",
        "    loss = .5 * (-torch.mean(torch.diagonal(torch.log_softmax(sim / torch.exp(model_CLIP.log_tau), dim=0))) \\\n",
        "                 -torch.mean(torch.diagonal(torch.log_softmax(sim / torch.exp(model_CLIP.log_tau), dim=1))))\n",
        "    return loss\n",
        "\n",
        "\"\"\" END OF THIS PART \"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Part 16 (5 points, coding task)\n",
        "\n",
        "In this part, you are asked to train your model.\n",
        "\n",
        "1. Set the number of epochs as 100.\n",
        "\n",
        "2. Do training on GPU.\n",
        "\n",
        "3. For every epoch, print the average loss per sample in this epoch.\n",
        "\n",
        "4. You may use `tqdm` to track your progress and help you manage your time.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### WRITE YOUR SOLUTION HERE ###\n",
        "\n",
        "num_epochs = 100\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "model_CLIP.to(device)\n",
        "\n",
        "for epoch in tqdm(range(num_epochs)):\n",
        "    model_CLIP.train()\n",
        "    optimizer.zero_grad()\n",
        "    loss_cum = 0\n",
        "    for image_batch, token_id_batch, attention_mask_batch in CLIP_dataloader:\n",
        "        image_batch = image_batch.to(device)\n",
        "        token_id_batch = token_id_batch.to(device)\n",
        "        attention_mask_batch = attention_mask_batch.to(device)\n",
        "        image_embedding, text_embedding = model_CLIP(image_batch, token_id_batch, attention_mask_batch)\n",
        "        loss = CLIP_loss_fn(image_embedding, text_embedding)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        loss_cum += loss.item() * image_batch.shape[0]\n",
        "    loss = loss_cum / len(CLIP_dateset)\n",
        "    print(f'Epoch {epoch}, Loss: {loss}')\n",
        "\n",
        "\"\"\" END OF THIS PART \"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "So far, we use the cosine function to measure the similarity between two vectors. Next, you are asked to do theoretical study of its reasonableness.\n",
        "\n",
        "Your task is to prove the following theorem.\n",
        "\n",
        "**Theorem:**\n",
        "\n",
        "Let $x, y \\in \\mathbb{R}^d$ be two independent $d$-dim vectors that follow the same multi-variate standard normal distribution $\\mathcal{N}(0_d, I_{d \\times d})$.\n",
        "\n",
        "Then for any $\\epsilon > 0$, when $d$ is large,\n",
        "\n",
        "$$P\\left( \\frac{x^\\top y}{\\|x\\|_2 \\|y\\|_2} > \\epsilon \\right) \\leq \\frac{1}{\\epsilon^2 d}.$$\n",
        "\n",
        "We prove this in multiple steps.\n",
        "\n",
        "---\n",
        "\n",
        "## Part 17 (5 points, non-coding task)\n",
        "\n",
        "First, you are asked to prove the following lemma.\n",
        "\n",
        "**Lemma 1:**\n",
        "\n",
        "If $x \\sim \\mathcal{N}(0_d, I_{d \\times d})$, then for any unit vector $\\hat{e} \\in \\mathbb{R}^d$,\n",
        "\n",
        "$$\\hat{e}^\\top x \\sim \\mathcal{N}(0, 1).$$\n",
        "\n",
        "That is, the projection of $x$ onto $\\hat{e}$ is a standard normal random variable.\n",
        "\n",
        "**Hint:** You can directly use the result that $\\hat{e}^\\top x$ is normal. Therefore, you only need to prove that $\\hat{e}^\\top x$ has mean 0 and variance 1.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Answer:**\n",
        "\n",
        "First, we have\n",
        "\n",
        "$$\\mathbb{E}[\\hat{e}^\\top x] = \\hat{e}^\\top \\mathbb{E}[x] = \\hat{e}^\\top 0_d = 0.$$\n",
        "\n",
        "Second, we have\n",
        "\n",
        "$$\\text{Var}[\\hat{e}^\\top x] = \\mathbb{E}[(\\hat{e}^\\top x)^2] - (\\mathbb{E}[\\hat{e}^\\top x])^2 = \\mathbb{E}[(\\hat{e}^\\top x)^2] = \\mathbb{E}[\\hat{e}^\\top x x^\\top \\hat{e}] = \\hat{e}^\\top \\mathbb{E}[x x^\\top] \\hat{e} = \\hat{e}^\\top I_{d \\times d} \\hat{e} = \\hat{e}^\\top \\hat{e} = 1.$$\n",
        "\n",
        "Therefore, $\\hat{e}^\\top x$ is a standard normal random variable.\n",
        "\n",
        "\"\"\" END OF THIS PART \"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Part 18 (5 points, non-coding task)\n",
        "\n",
        "Lemma 1 implies that the projection of $x$ onto any direction is a standard normal. Therefore, all directions are homogeneous.\n",
        "\n",
        "Therefore,\n",
        "\n",
        "$$P\\left( \\frac{x^\\top y}{\\|x\\|_2 \\|y\\|_2} > \\epsilon \\right) = P\\left( \\frac{x^\\top y}{\\|x\\|_2 \\|y\\|_2} > \\epsilon \\mid x = \\hat{x} \\right), \\quad \\forall \\hat{x} \\in \\mathbb{R}^d.$$\n",
        "\n",
        "For simplicity, we consider\n",
        "\n",
        "$$\\hat{x} = \\begin{bmatrix} 1 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{bmatrix} \\in \\mathbb{R}^d.$$\n",
        "\n",
        "Therefore, we only need to bound\n",
        "\n",
        "$$P\\left( \\frac{y_0}{\\|y\\|_2} > \\epsilon \\right)$$\n",
        "\n",
        "By symmetry, it is easy to see that\n",
        "\n",
        "$$\\mathbb{E}\\left[ \\frac{y_0}{\\|y\\|_2} \\right] = 0.$$\n",
        "\n",
        "Hence, we get\n",
        "\n",
        "$$P\\left( \\frac{y_0}{\\|y\\|_2} > \\epsilon \\right) \\leq \\frac{\\text{Var}\\left[ \\frac{y_0}{\\|y\\|_2} \\right]}{\\epsilon^2} = \\frac{\\mathbb{E}\\left[ \\frac{y_0^2}{\\|y\\|_2^2} \\right]}{\\epsilon^2}$$\n",
        "\n",
        "$$= \\frac{1}{\\epsilon^2 d} \\mathbb{E}\\left[ \\frac{y_0^2}{\\frac{1}{d} \\sum_{i=0}^{d-1} y_i^2} \\right]$$\n",
        "\n",
        "where the first inequality follows from the Chebyshev's inequality.\n",
        "\n",
        "To prove the theorem, it is equivalent to prove the following lemma.\n",
        "\n",
        "**Lemma 2:**\n",
        "\n",
        "Let $y_0, \\cdots, y_{d-1}$ be identically and independent variables that are all standard normals. Then for large $d$,\n",
        "\n",
        "$$\\mathbb{E}\\left[ \\frac{y_0^2}{\\frac{1}{d} \\sum_{i=0}^{d-1} y_i^2} \\right] \\approx 1.$$\n",
        "\n",
        "In this part, your task is to prove this lemma.\n",
        "\n",
        "**Hint:** It is hard to prove this statement in an exact way. You can make any reasonable approximation.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Answer:**\n",
        "\n",
        "Since $y_i$ is a standard normal, $\\mathbb{E}[y_i^2] = 1$ and $\\text{Var}[y_i^2] = 2$.\n",
        "\n",
        "For large $d$, the central limit theorem implies\n",
        "\n",
        "$$\\frac{1}{d} \\sum_{i=0}^{d-1} y_i^2 \\sim \\mathcal{N}\\left(1, \\frac{2}{d}\\right).$$\n",
        "\n",
        "Hence, for large $d$, $\\frac{1}{d} \\sum_{i=0}^{d-1} y_i^2$ can be approximated as its mean value, 1.\n",
        "\n",
        "Therefore, for large $d$,\n",
        "\n",
        "$$\\mathbb{E}\\left[ \\frac{y_0^2}{\\frac{1}{d} \\sum_{i=0}^{d-1} y_i^2} \\right] \\approx \\mathbb{E}[y_0^2] = 1.$$\n",
        "\n",
        "\"\"\" END OF THIS PART \"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Part 19 (5 points, non-coding task)\n",
        "\n",
        "Lemmas 1 and 2 jointly imply the theorem above. Please use the result in this theorem to explain why it is reasonable to use the cosine function to measure similarity of two embedding vectors and why the latent space needs to be high dimensional (such as 512, 768, 1024).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Answer:**\n",
        "\n",
        "The theorem states that in a high dimensional space, almost all pairs of vectors are orthogonal (independent), except very few that are in the same direction.\n",
        "\n",
        "This is exactly what we want in matching images and texts. For instance, suppose there are 30k pairs of iamges and texts. For each image embedding vector, we want it to be aligned with only one text embedding vector, but orthogonal to other 30k-1 text embedding vectors. This is guaranteed by the above theorem.\n",
        "\n",
        "Recall that a key condition of the above theorem is that the dimension must be high. Therefore, in image and text embeddings, the embedded vectors must be high dimensional.\n",
        "\n",
        "\"\"\" END OF THIS PART \"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Part 20 (5 points, non-coding task)\n",
        "\n",
        "In the loss function, we introduced a crutial learnable parameter $\\tau$, called temperature.\n",
        "\n",
        "Let us explore some properties of $\\tau$.\n",
        "\n",
        "Let $z_0 > z_1 > \\cdots > z_{N-1}$.\n",
        "\n",
        "Define\n",
        "\n",
        "$$f_i = \\frac{\\exp(z_i / \\tau)}{\\sum_{j=0}^{N-1} \\exp(z_j / \\tau)}.$$\n",
        "\n",
        "Do the following analysis. Reasoning is required.\n",
        "\n",
        "1. Compute $\\lim_{\\tau \\to 0^+} f_i$.\n",
        "\n",
        "2. Compute $\\lim_{\\tau \\to \\infty} f_i$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Answer:**\n",
        "\n",
        "1. We have\n",
        "\n",
        "$$\\lim_{\\tau \\to 0^+} f_i = \\lim_{\\tau \\to 0^+} \\frac{\\exp(z_i / \\tau)}{\\sum_{j=0}^{N-1} \\exp(z_j / \\tau)} = \\lim_{\\tau \\to 0^+} \\frac{\\exp((z_i - z_0) / \\tau)}{\\sum_{j=0}^{N-1} \\exp((z_j - z_0) / \\tau)} = \\begin{cases} 1 & \\text{if } i = 0 \\\\ 0 & \\text{if } i \\neq 0 \\end{cases}.$$\n",
        "\n",
        "2. We have\n",
        "\n",
        "$$\\lim_{\\tau \\to \\infty} f_i = \\lim_{\\tau \\to \\infty} \\frac{\\exp(z_i / \\tau)}{\\sum_{j=0}^{N-1} \\exp(z_j / \\tau)} = \\frac{1}{\\sum_{j=0}^{N-1} 1} = \\frac{1}{N}.$$\n",
        "\n",
        "\"\"\" END OF THIS PART \"\"\"\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
