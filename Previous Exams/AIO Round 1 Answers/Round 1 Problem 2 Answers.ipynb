{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2025 USA-NA-AIO Round 1, Problem 2 â€” ANSWERS\n",
        "\n",
        "## Problem 2 (100 points)\n",
        "\n",
        "This problem is about the basics of neural network.\n",
        "\n",
        "Before starting this problem, make sure to run the following code first without change:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\"\"\" DO NOT CHANGE \"\"\"\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "torch.manual_seed(2025)\n",
        "\n",
        "\"\"\" END OF THIS PART \"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## $\\color{red}{\\text{WARNING !!!}}$\n",
        "\n",
        "Beyond importing libraries/modules/classes/functions in the preceding cell, you are **NOT** allowed to import anything else for the following purposes:\n",
        "\n",
        "- As a part of your final solution. For instance, if a problem asks you to build a model without using sklearn but you use it, then you will not earn points.\n",
        "\n",
        "- Temporarily import something to assist you to get a solution. For instance, if a problem asks you to manually compute eigenvalues but you temporarily use `np.linalg.eig` to get an answer and then delete your code, then you violate the rule.\n",
        "\n",
        "**Rule of thumb:** Each part has its particular purpose to intentionally test you something. Do not attempt to find a shortcut to circumvent the rule.\n",
        "\n",
        "All coding tasks shall run on **CPUs, not GPUs**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Part 1 (5 points, non-coding task)\n",
        "\n",
        "The high level idea of affine transformation in math is that for each column vector $\\mathbf{x} \\in \\mathbb{R}^N$, an affine transformation maps it to another column vector $\\mathbf{y} \\in \\mathbb{R}^M$ via\n",
        "\n",
        "$$\\mathbf{y} = \\mathbf{W} \\mathbf{x} + \\mathbf{b}$$\n",
        "\n",
        "where\n",
        "\n",
        "- $\\mathbf{W} \\in \\mathbb{R}^{M \\times N}$.\n",
        "- $\\mathbf{b} \\in \\mathbb{R}^{M}$.\n",
        "\n",
        "Now, let us study a small-sized problem.\n",
        "\n",
        "Let\n",
        "\n",
        "$$\\mathbf{W} = \\begin{bmatrix} 2 & -3 & 1 & 3 & -2 \\\\ 0 & 1 & 2 & 5 & -1 \\\\ 7 & -1 & -3 & 7 & 0 \\end{bmatrix}$$\n",
        "\n",
        "and\n",
        "\n",
        "$$\\mathbf{b} = \\begin{bmatrix} 1 \\\\ 0 \\\\ -1 \\end{bmatrix}$$\n",
        "\n",
        "and\n",
        "\n",
        "$$\\mathbf{x} = \\begin{bmatrix} 1 \\\\ 2 \\\\ -3 \\\\ 1 \\\\ -2 \\end{bmatrix}$$\n",
        "\n",
        "Answer the following questions:\n",
        "\n",
        "1. What is the value of $N$?\n",
        "2. What is the value of $M$?\n",
        "3. What is the value of $\\mathbf{y}$?\n",
        "\n",
        "Questions 1 and 2 do not require reasoning. Question 3 requires reasoning.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Answer:**\n",
        "\n",
        "1. $N = 5$.\n",
        "\n",
        "2. $M = 3$.\n",
        "\n",
        "3. We have\n",
        "\n",
        "$$\\mathbf{y} = \\mathbf{W}\\mathbf{x} + \\mathbf{b} = \\begin{bmatrix} 2 & -3 & 1 & 3 & -2 \\\\ 0 & 1 & 2 & 5 & -1 \\\\ 7 & -1 & -3 & 7 & 0 \\end{bmatrix} \\begin{bmatrix} 1 \\\\ 2 \\\\ -3 \\\\ 1 \\\\ -2 \\end{bmatrix} + \\begin{bmatrix} 1 \\\\ 0 \\\\ -1 \\end{bmatrix}$$\n",
        "\n",
        "$$= \\begin{bmatrix} 2 \\cdot 1 + (-3) \\cdot 2 + 1 \\cdot (-3) + 3 \\cdot 1 + (-2) \\cdot (-2) + 1 \\\\ 0 \\cdot 1 + 1 \\cdot 2 + 2 \\cdot (-3) + 5 \\cdot 1 + (-1) \\cdot (-2) + 0 \\\\ 7 \\cdot 1 + (-1) \\cdot 2 + (-3) \\cdot (-3) + 7 \\cdot 1 + 0 \\cdot (-2) + (-1) \\end{bmatrix}$$\n",
        "\n",
        "$$= \\begin{bmatrix} 1 \\\\ 3 \\\\ 20 \\end{bmatrix}$$\n",
        "\n",
        "\"\"\" END OF THIS PART \"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Part 2 (10 points, non-coding task)\n",
        "\n",
        "Define $\\nabla_{\\mathbf{z}} f(\\mathbf{z})$ to be the gradient of function $f$ with respect to vector/matrix $\\mathbf{z}$.\n",
        "\n",
        "Compute the following gradients. Reasoning is required.\n",
        "\n",
        "1. $\\nabla_{\\mathbf{x}} \\mathbf{y}$.\n",
        "   - The final answer should be in a matrix form.\n",
        "\n",
        "2. $\\nabla_{\\mathbf{W}} \\mathbf{y}$.\n",
        "   - The final answer should be in an element-wise form.\n",
        "\n",
        "3. $\\nabla_{\\mathbf{b}} \\mathbf{y}$.\n",
        "   - The final answer should be in a matrix form.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Answer:**\n",
        "\n",
        "**1.** Since $\\mathbf{y} \\in \\mathbb{R}^M$ and $\\mathbf{x} \\in \\mathbb{R}^N$, $\\nabla_{\\mathbf{x}} \\mathbf{y} \\in \\mathbb{R}^{M \\times N}$.\n",
        "\n",
        "We have\n",
        "\n",
        "$$\\frac{\\partial y_m}{\\partial x_n} = \\frac{\\partial \\left( \\sum_{i=0}^{N-1} w_{mi} x_i + b_m \\right)}{\\partial x_n} = w_{mn}$$\n",
        "\n",
        "Therefore,\n",
        "\n",
        "$$\\nabla_{\\mathbf{x}} \\mathbf{y} = \\mathbf{W}$$\n",
        "\n",
        "\n",
        "\n",
        "**2.** Since $\\mathbf{y} \\in \\mathbb{R}^M$ and $\\mathbf{W} \\in \\mathbb{R}^{M \\times N}$, $\\nabla_{\\mathbf{W}} \\mathbf{y} \\in \\mathbb{R}^{M \\times M \\times N}$.\n",
        "\n",
        "We have\n",
        "\n",
        "$$\\frac{\\partial y_m}{\\partial w_{kn}} = \\frac{\\partial \\left( \\sum_{i=0}^{N-1} w_{mi} x_i + b_m \\right)}{\\partial w_{kn}} = x_n \\delta_{mk}$$\n",
        "\n",
        "where $\\delta_{mk}$ is the Kronecker delta (equals 1 if $m = k$, else 0).\n",
        "\n",
        "\n",
        "\n",
        "**3.** Since $\\mathbf{y} \\in \\mathbb{R}^M$ and $\\mathbf{b} \\in \\mathbb{R}^M$, $\\nabla_{\\mathbf{b}} \\mathbf{y} \\in \\mathbb{R}^{M \\times M}$.\n",
        "\n",
        "We have\n",
        "\n",
        "$$\\frac{\\partial y_m}{\\partial b_k} = \\frac{\\partial \\left( \\sum_{i=0}^{N-1} w_{mi} x_i + b_m \\right)}{\\partial b_k} = \\delta_{mk}$$\n",
        "\n",
        "Therefore,\n",
        "\n",
        "$$\\nabla_{\\mathbf{b}} \\mathbf{y} = \\mathbf{I}_{M \\times M}$$\n",
        "\n",
        "\"\"\" END OF THIS PART \"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Part 3 (10 points, coding task)\n",
        "\n",
        "In this part, you are asked to build an affine transformation module from scratch by using NumPy, **NOT** PyTorch or TensorFlow.\n",
        "\n",
        "Define such a class as `My_Linear_NumPy`.\n",
        "\n",
        "**Attributes:**\n",
        "\n",
        "- `in_features`: Number of input features\n",
        "- `out_features`: Number of output features\n",
        "- `weight`: This refers to matrix $\\mathbf{W}$ in Part 1. The shape is `(out_features, in_features)`.\n",
        "- `bias`: This refers to vector $\\mathbf{b}$ in Part 1. The shape is `(out_features,)`.\n",
        "- `random_seed`: The NumPy random seed number used to generate initial values of weight and bias.\n",
        "\n",
        "**Method `__init__`:**\n",
        "\n",
        "- To initialize an object in this class, you need to specify `in_features` and `out_features`.\n",
        "- You may initialize the object by specifying a value for `random_seed`. If it is not specified, then its default value is `42`.\n",
        "- The initial values of `weight` and `bias` are random that follow standard normal distributions generated with the seed number attribute `random_seed`.\n",
        "\n",
        "**Method `forward`:**\n",
        "\n",
        "- **Input** `x`: numpy array with shape `(n_0, n_1, ..., n_{d-1}, in_features)` with an arbitrary dimension $d = 0, 1, \\cdots$.\n",
        "- **Output** `y`: numpy array with shape `(n_0, n_1, ..., n_{d-1}, out_features)`.\n",
        "- The affine transformation works in a way that given the first $d$ indices in `x` and `y`, it does affine transformation along the last axis of `x` and `y`.\n",
        "- Do not use any loop in your code.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### WRITE YOUR SOLUTION HERE ###\n",
        "\n",
        "class My_Linear_NumPy:\n",
        "    def __init__(self, in_features, out_features, random_seed=42):\n",
        "        self.in_features = in_features\n",
        "        self.out_features = out_features\n",
        "        self.random_seed = random_seed\n",
        "        np.random.seed(self.random_seed)\n",
        "        self.weight = np.random.randn(self.out_features, self.in_features)\n",
        "        self.bias = np.random.randn(self.out_features)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return np.sum(x[..., np.newaxis] * self.weight.T, axis = -2) + self.bias\n",
        "\n",
        "\"\"\" END OF THIS PART \"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Part 4 (5 points, coding task)\n",
        "\n",
        "Do the following tasks in this part.\n",
        "\n",
        "1. Construct an object in the class `My_Linear_NumPy` called `linear_model_np`.\n",
        "2. Set `in_features = 3` and `out_features = 5`.\n",
        "3. Create multiple `X` with the following different shapes, but common numpy random seed number `2025` and the same standard normal distribution.\n",
        "    - `(in_features,)`\n",
        "    - `(10, in_features)`\n",
        "    - `(10, 20, in_features)`\n",
        "    - `(10, 20, 30, in_features)`\n",
        "    - After generating `X`, reset the numpy random seed number to its default value.\n",
        "4. We call our constructed function with each of the above `X` as the input. Print the shape of each output.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### WRITE YOUR SOLUTION HERE ###\n",
        "\n",
        "in_features = 3\n",
        "out_features = 5\n",
        "\n",
        "linear_model_np = My_Linear_NumPy(in_features, out_features)\n",
        "\n",
        "np.random.seed(2025)\n",
        "\n",
        "X_list = [np.random.randn(in_features), np.random.randn(10,in_features), np.random.randn(10,20,in_features), np.random.randn(10,20,30,in_features)]\n",
        "\n",
        "np.random.seed()\n",
        "\n",
        "for X in X_list:\n",
        "    print(linear_model_np.forward(X).shape)\n",
        "\n",
        "\"\"\" END OF THIS PART \"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Part 5 (10 points, coding task)\n",
        "\n",
        "In this part, you are asked to program with PyTorch, **NOT** NumPy.\n",
        "\n",
        "Define a deep neural network module (class) named `Linear_Model`.\n",
        "\n",
        "It has the following architecture:\n",
        "\n",
        "- 2 layers: 1 hidden layer and 1 output layer.\n",
        "- No activation function. That is, the connection between two consecutive layers is only an affine transformation.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### WRITE YOUR SOLUTION HERE ###\n",
        "\n",
        "class Linear_Model(nn.Module):\n",
        "    def __init__(self, in_features, hidden_features, out_features):\n",
        "        super().__init__()\n",
        "        self.in_features = in_features\n",
        "        self.hidden_features = hidden_features\n",
        "        self.out_features = out_features\n",
        "        self.linear0 = nn.Linear(in_features, hidden_features)\n",
        "        self.linear1 = nn.Linear(hidden_features, out_features)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear0(x)\n",
        "        x = self.linear1(x)\n",
        "        return x\n",
        "\n",
        "\"\"\" END OF THIS PART \"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Part 6 (5 points, non-coding task)\n",
        "\n",
        "We make the following modifications on the previous part.\n",
        "\n",
        "- We consider a special symmetric neural network that `out_features = in_features`.\n",
        "- No bias in all affine transformations.\n",
        "- The transformation matrix from the hidden layer to the output layer is binded to be the transpose of the transformation matrix from the input layer to the hidden layer.\n",
        "\n",
        "What is the total number of learnable parameters in this model?\n",
        "\n",
        "Reasoning is not required.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Answer:**\n",
        "\n",
        "$\\text{in\\_features} \\times \\text{hidden\\_features}$\n",
        "\n",
        "\"\"\" END OF THIS PART \"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Part 7 (5 points, non-coding task)\n",
        "\n",
        "This question follows Part 6.\n",
        "\n",
        "Denote by $\\mathbf{W} \\in \\mathbb{R}^{\\text{hidden\\_features} \\times \\text{input\\_features}}$ the transformation matrix from the input layer to the hidden layer.\n",
        "\n",
        "Given input $\\mathbf{x} \\in \\mathbb{R}^{\\text{input\\_features}}$, write down the formula of the output $\\mathbf{y}$ and its shape.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Answer:**\n",
        "\n",
        "$$\\mathbf{y} = \\mathbf{W}^\\top \\mathbf{W} \\mathbf{x} \\in \\mathbb{R}^{\\text{input\\_features}}$$\n",
        "\n",
        "\"\"\" END OF THIS PART \"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Part 8 (10 points, non-coding task)\n",
        "\n",
        "This question follows Part 7.\n",
        "\n",
        "Denote $r = \\text{rank} \\left( \\mathbf{W} \\right)$.\n",
        "\n",
        "Compute the rank of $\\mathbf{W}^\\top \\mathbf{W}$.\n",
        "\n",
        "Reasoning is required.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Answer:**\n",
        "\n",
        "Let $\\mathbf{W}$ be with the following singular value decomposition:\n",
        "\n",
        "$$\\mathbf{W} = \\sum_{i=0}^{r-1} \\mathbf{u}_i \\sigma_i \\mathbf{v}_i^\\top$$\n",
        "\n",
        "Hence,\n",
        "\n",
        "$$\\mathbf{W}^\\top \\mathbf{W} = \\left( \\sum_{i=0}^{r-1} \\mathbf{u}_i \\sigma_i \\mathbf{v}_i^\\top \\right)^\\top \\left( \\sum_{j=0}^{r-1} \\mathbf{u}_j \\sigma_j \\mathbf{v}_j^\\top \\right)$$\n",
        "\n",
        "$$= \\sum_{i=0}^{r-1} \\sum_{j=0}^{r-1} \\mathbf{v}_i \\sigma_i \\mathbf{u}_i^\\top \\mathbf{u}_j \\sigma_j \\mathbf{v}_j^\\top$$\n",
        "\n",
        "$$= \\sum_{i=0}^{r-1} \\sum_{j=0}^{r-1} \\mathbf{v}_i \\sigma_i \\sigma_j \\delta_{ij} \\mathbf{v}_j^\\top$$\n",
        "\n",
        "$$= \\sum_{i=0}^{r-1} \\mathbf{v}_i \\sigma_i^2 \\mathbf{v}_i^\\top$$\n",
        "\n",
        "This is the singular value decomposition of $\\mathbf{W}^\\top \\mathbf{W}$. Therefore, the rank of this matrix is also $r$.\n",
        "\n",
        "\"\"\" END OF THIS PART \"\"\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Part 9 (5 points, coding task)\n",
        "\n",
        "This question follows Part 6.\n",
        "\n",
        "In this part, you are asked to program with PyTorch, **NOT** NumPy.\n",
        "\n",
        "Build a deep neural network class named as `Symmetric_Linear_Model` that meets the modifications imposed in Part 6.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### WRITE YOUR SOLUTION HERE ###\n",
        "\n",
        "class Symmetric_Linear_Model(nn.Module):\n",
        "    def __init__(self, in_features, hidden_features):\n",
        "        super().__init__()\n",
        "        self.in_features = in_features\n",
        "        self.hidden_features = hidden_features\n",
        "        self.linear0 = nn.Linear(in_features, hidden_features, bias = False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.linear0(x)\n",
        "        x = torch.sum(self.linear0.weight * x.reshape(*x.shape, 1), dim = -2)\n",
        "        return x\n",
        "\n",
        "\"\"\" END OF THIS PART \"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Part 10 (5 points, coding task)\n",
        "\n",
        "Rectified Linear Unit, or the \"ReLU\", is one of the most common used function in deep learning. It is defined as\n",
        "\n",
        "$$\\text{ReLU}(x) = \\max \\left\\{ 0, x \\right\\}$$\n",
        "\n",
        "In this part, you are asked to use PyTorch to build a ReLU class named `My_ReLU` that subclasses `nn.Module`.\n",
        "\n",
        "A successful class works in the following ways:\n",
        "\n",
        "- The initialization of an object in `My_ReLU` does not take any input.\n",
        "- Suppose we have a `My_ReLU` object called `activation0`. When we call `activation0(x)` with input `x` that is a tensor with an arbitrary dimension and shape, we get an output `y` from the element-wise ReLU activation on `x`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### WRITE YOUR SOLUTION HERE ###\n",
        "\n",
        "class My_ReLU(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.max(torch.zeros_like(x), x)\n",
        "\n",
        "\"\"\" END OF THIS PART \"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Part 11 (10 points, coding task)\n",
        "\n",
        "It is known by math that the combination of several linear layers can still be seen as a linear layer, so we can add some non-linear activation functions, such as ReLU, in between to get better effect.\n",
        "\n",
        "Multi-Layer Perceptron (MLP), is such a neural network composed of multiple fully connected layers with non-linear activations, commonly used in deep learning.\n",
        "\n",
        "Please define a class called `My_MLP_Model` that subclasses `nn.Module` and works in the following ways:\n",
        "\n",
        "- The architecture consists of two hidden layers and one output layer.\n",
        "- Each hidden layer consists of an affine transformation module and a ReLU activation module.\n",
        "- Each affine transformation module shall be initialized with the built-in class `nn.Linear`.\n",
        "- Each ReLU activation module shall be initialized with your self-defined class `My_ReLU`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### WRITE YOUR SOLUTION HERE ###\n",
        "\n",
        "class My_MLP_Model(nn.Module):\n",
        "    def __init__(self, in_features, hidden_features1, hidden_features2, out_features):\n",
        "        super().__init__()\n",
        "        self.in_features = in_features\n",
        "        self.hidden_features1 = hidden_features1\n",
        "        self.hidden_features2 = hidden_features2\n",
        "        self.out_features = out_features\n",
        "        self.seq = nn.Sequential(\n",
        "            nn.Linear(in_features, hidden_features1),\n",
        "            My_ReLU(),\n",
        "            nn.Linear(hidden_features1, hidden_features2),\n",
        "            My_ReLU(),\n",
        "            nn.Linear(hidden_features2, out_features)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.seq(x)\n",
        "        return x\n",
        "\n",
        "\"\"\" END OF THIS PART \"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Part 12 (5 points, coding task)\n",
        "\n",
        "After building our deep neural network architecture in Part 11 and before using it to train our model, we need to prepare our training dataset.\n",
        "\n",
        "Let us look at a simple application of deep neural network in studying harmonic motion in physics.\n",
        "\n",
        "Write code to construct the following training dataset:\n",
        "\n",
        "- Use `sample_size` to store the number of samples. Set the value as `1000`.\n",
        "- Define `x_train` as a tensor whose shape is `(sample_size,)` and the value on each entry is uniformly drawn between 0 and 1.\n",
        "- Define `y_train` as a tensor whose values are obtained from the following element-wise mapping from `x_train`:\n",
        "\n",
        "$$y = \\sin \\left( 2 \\pi x \\right) + 0.1 \\cdot \\mathcal{N} \\left( 0, 1 \\right)$$\n",
        "\n",
        "where $\\mathcal{N} \\left( 0, 1 \\right)$ is a standard normal random variable.\n",
        "\n",
        "- Print the dimensions of `x_train` and `y_train`.\n",
        "- Print the shapes of `x_train` and `y_train`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "### WRITE YOUR SOLUTION HERE ###\n",
        "\n",
        "sample_size = 1000\n",
        "x_train = torch.rand(sample_size,)\n",
        "y_train = torch.sin(2 * np.pi * x_train) + 0.1 * torch.randn_like(x_train)\n",
        "\n",
        "print(x_train.ndim)\n",
        "print(y_train.ndim)\n",
        "\n",
        "print(x_train.shape)\n",
        "print(y_train.shape)\n",
        "\n",
        "\"\"\" END OF THIS PART \"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Part 13 (15 points, coding task)\n",
        "\n",
        "In this part, we use the training dataset constructed in Part 12 to train a model defined in Part 11.\n",
        "\n",
        "- Use mean-squared error (MSE) as the loss function.\n",
        "- Use Adam as the optimization algorithm.\n",
        "- Do whole-batch training in each epoch.\n",
        "- After every 10 epochs, print the following sentence:\n",
        "\n",
        "`Epoch: XXX. Loss: XXX.`\n",
        "\n",
        "The loss value should be with 4 decimal places.\n",
        "\n",
        "- Generate an epoch-MSE loss plot after completing the training. Set the x-label as `epoch` and the y-label as `MSE loss`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# HYPERPARAMETERS\n",
        "''' DO NOT CHANGE ANYTHING IN THIS CODE CELL '''\n",
        "\n",
        "hidden_features1 = 32\n",
        "hidden_features2 = 16\n",
        "\n",
        "num_epochs = 500\n",
        "learning_rate = 1e-3\n",
        "\n",
        "### WRITE YOUR SOLUTION HERE ###\n",
        "\n",
        "my_mlp_model = My_MLP_Model(1, hidden_features1, hidden_features2, 1)\n",
        "optimizer = torch.optim.Adam(my_mlp_model.parameters(), lr = learning_rate)\n",
        "loss_fn = torch.nn.MSELoss()\n",
        "\n",
        "loss_list_plot = []\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    optimizer.zero_grad()\n",
        "    y_pred = my_mlp_model(x_train.reshape(-1,1))\n",
        "    loss = loss_fn(y_pred, y_train.reshape(-1,1))\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        print(f\"Epoch: {epoch}. Loss: {loss.item():.4f}\")\n",
        "        loss_list_plot.append(loss.item())\n",
        "\n",
        "plt.plot(loss_list_plot)\n",
        "plt.xlabel(\"epoch\")\n",
        "plt.ylabel(\"MSE loss\")\n",
        "plt.show()\n",
        "\n",
        "\"\"\" END OF THIS PART \"\"\""
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
