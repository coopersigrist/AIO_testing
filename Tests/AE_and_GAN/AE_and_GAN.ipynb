{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "365879a4",
   "metadata": {},
   "source": [
    "## Instructions!\n",
    "This time, we'll be using PyTorch to implement two types of neural network -- these are pretty cool.\n",
    "\n",
    "The first of these is going to be an *autoencoder*.  An autoencoder is a neural network with a pretty unique structure, which will learn a function that can map an input to itself.  This allows the network to extract important features from an input to effectively compress it, and then reconstruct those important features back into something that approximates the original input closely.\n",
    "\n",
    "The second of these is going to be a *GAN (generative adversarial network)*, which is a framework where we train two neural networks - one learns to generate synthetic data based on training data (the generator), and the other learns to distinguish true data from generated data (the discriminator).  This is where the word *adversarial* comes into the picture -- as the discriminator gets better at identifying fake data, the feedback is passed to the generator, which gets better at creating synthetic data.\n",
    "\n",
    "One example of a cutting edge GAN that you can quickly check out is [This Person Does Not Exist](https://thispersondoesnotexist.com), which generates synthetic portraits of people!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a19048c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Let's import some libraries!\n",
    "import time\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, Subset\n",
    "from torchvision import datasets\n",
    "from torchvision.datasets import ImageFolder\n",
    "import torchvision.transforms as tt\n",
    "from torchvision.utils import make_grid\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.image import imread\n",
    "import os\n",
    "import random\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014ef897",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a new dependency to load this dataset, so make sure you install this module too\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "463bdd09",
   "metadata": {},
   "source": [
    "## Our Dataset\n",
    "For this project we're going to use **human faces**! Well, *images* of human faces, but it's still cool. \n",
    "\n",
    "The particular dataset is calleb **CelebA** -- you can find the official website [here](https://mmlab.ie.cuhk.edu.hk/projects/CelebA.html) and you can find the torchvision documentation [here](http://pytorch.org/vision/main/generated/torchvision.datasets.CelebA.html)\n",
    "\n",
    "Here's what the dataset looks like to us humans:\n",
    "\n",
    "<center><img src=\"https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Fwww.researchgate.net%2Fpublication%2F325008881%2Ffigure%2Fdownload%2Ffig2%2FAS%3A623857358602242%401525750593608%2FVisual-Inspection-CelebA-dataset-64x64-samples-from-MEGAN-with-each-block-of-four.png&f=1&nofb=1\" width=\"624\" height=\"424\" /></center>\n",
    "\n",
    "\n",
    "They're a bit low res, but they'll still work!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba2d1851",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_celebA(batch_size=32, train=True):\n",
    "\n",
    "    '''\n",
    "    Dataset loading will be handled for you automatically since it's a bit of a pain\n",
    "    to work with these large datasets and I'll just give you a subset\n",
    "    '''\n",
    "\n",
    "    dataset = []\n",
    "    batch_counter = 0\n",
    "    batch = []\n",
    "\n",
    "    for file in tqdm(os.listdir('./celebA')):\n",
    "        \n",
    "        img = Image.open('./celebA/' + file).resize((89, 109))\n",
    "        img = np.asarray(img).reshape(3, 109, 89)\n",
    "\n",
    "        if batch_counter < batch_size:\n",
    "            batch.append(img)\n",
    "            batch_counter += 1\n",
    "        else:\n",
    "            dataset.append(np.array(batch))\n",
    "            batch = []\n",
    "            batch_counter = 0\n",
    "\n",
    "    return np.array(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0f4e6eb",
   "metadata": {},
   "source": [
    "### Now let's see what our data looks like!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009d4354",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image(image):\n",
    "    \n",
    "    '''\n",
    "    Takes in an image and shows it using matplotlib \n",
    "    this is used to visualize the data and also the outputs of our network\n",
    "    '''\n",
    "\n",
    "    image = image.reshape(-1, 109,89,3)\n",
    "\n",
    "    plt.imshow(image[0])\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff1c254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will load the dataset and set the dataset variable to it\n",
    "# Try to only run this code once since it takes a while (though you may again to change the batch size)\n",
    "\n",
    "dataset = load_celebA(batch_size=32, train=True)\n",
    "dataset = torch.from_numpy(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40b7bfe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This just displays a random image from the dataset \n",
    "ex_image = dataset[random.randint(0,100)]\n",
    "print(\"image shape:\", ex_image.shape)\n",
    "\n",
    "plot_image(ex_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc598db5",
   "metadata": {},
   "source": [
    "## Autoencoder (40 points)\n",
    "\n",
    "Just like we did in HW3, we're doing to build a PyTorch ```nn.Module``` for our autoencoder, but this time it will combine two other ```nn.modules``` -- an Encoder and Decoder. Your job will be to implement these two modules, we'll combine them for you cause I'm nice. Again, each module consists of 2 parts: The initialization (defined in ```__init__()``` -- note that this the python convention for initalizing classes) and the forward pass (defined aptly as ```forward()```)\n",
    "\n",
    "Since we're using PyTorch, we can simply define this module and then the gradient can be found *automatically*.\n",
    "\n",
    "Documentation for a pytorch module can be found [here](https://pytorch.org/docs/stable/generated/torch.nn.Module.html)\n",
    "\n",
    "You can use any building-block modules you'd like from the nn library -- particularly these:\n",
    "```nn.Linear()``` documentation is [here](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html);\n",
    " ```nn.ReLU()``` documentation is [here](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html);\n",
    " ```nn.Conv2d()``` documentation is [here](https://pytorch.org/docs/stable/generated/torch.nn.Conv2d.html);\n",
    " ```nn.ConvTranspose2d()``` (this is the inverse of a convolution) documentation is [here](https://pytorch.org/docs/stable/generated/torch.nn.ConvTranspose2d.html);\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e57e86",
   "metadata": {},
   "source": [
    "### Autoencoder Model (10 points)\n",
    "\n",
    "Here you are going to make the module for your encoder and decoder -- the encoder will take an image as an input and compress it to some size (its a hyperparameter) and then the decoder will take that compressed (latent) representation of the image and try to reconstruct the original"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b29941",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "\n",
    "    '''\n",
    "    This will be the module for your encoder half of your autoencoder\n",
    "    You may use Linear layers, conv2d layers and anything else you'd like (you just need the first two)\n",
    "\n",
    "    One thing that might throw you is that input_shape is going to be a tuple which represents the \n",
    "    entire shape of the input (i.e. an image is (3,109,89) in CelebA)\n",
    "    '''\n",
    "\n",
    "    def __init__(self, input_shape, compression_size):\n",
    "        super().__init__()\n",
    "\n",
    "\n",
    "        # TODO initialize the Encoder network \n",
    "        # You must use nn.Conv2d, nn.Linear, and nn.ReLU but you can look for anything else you'd like to use\n",
    "        # Hint you may also want to define an nn.Flatten()\n",
    "\n",
    "        #################################################### \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #################################################### \n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = x.view(-1, 3, 109, 89).float() # we convert from byte to float cause we need floats for torch operations\n",
    "        # This also reshapes the input so that the color dimension (channels) is in the right place for our operations\n",
    "        \n",
    "        #################################################### \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #################################################### \n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbfdbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder((3,109,89), 100)\n",
    "test_out = encoder(ex_image)\n",
    "\n",
    "print(test_out.shape)\n",
    "print(\"the shape of the output should be a vector of size batch_size,100, is it?\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4aa80d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    '''\n",
    "    This is the other bit of the autoencoder\n",
    "    Likewise you can use whatever you'd like to get from the output of the encoder\n",
    "    (which should be a vector )\n",
    "    '''\n",
    "\n",
    "    def __init__(self, input_size, output_shape):\n",
    "        super().__init__()\n",
    "\n",
    "        \n",
    "\n",
    "        self.output_shape = output_shape\n",
    "\n",
    "        # TODO initialize your Decoder\n",
    "        # You can use Linear or conv layers as you'd like, but since we are expanding the size\n",
    "        # You may want to look into deconvolution, which is called nn.Conv2Transpose\n",
    "\n",
    "        ###############################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        ###############################################\n",
    "\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # TODO finish the forward pass of your Decoder\n",
    "        # Input is the output of the encoder\n",
    "\n",
    "        ###############################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        ###############################################\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6fe620",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Decoder(100, (3,109,89))\n",
    "test_out = decoder(torch.from_numpy(np.ones((32,100))).float())\n",
    "\n",
    "print(test_out.shape)\n",
    "print(\"the shape of the output should be shape (batch_size,3,109,89), is it?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fada9d60",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Autoencoder(nn.Module):\n",
    "\n",
    "    '''\n",
    "    This will combine your encoder and decoder modules together\n",
    "    with their powers combined they make an AUTOENCODER\n",
    "\n",
    "    You may have issue with the shape of the input to the decoder\n",
    "    remember that we pass in compression_size which will just be an int\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, input_shape, compression_size):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_size = input_shape\n",
    "\n",
    "        self.encoder = Encoder(input_shape, compression_size)\n",
    "        self.decoder = Decoder(compression_size, input_shape)\n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "        \n",
    "\n",
    "    def forward(self, features):\n",
    "        \n",
    "        out = self.encoder(features)\n",
    "        out = self.relu(out)\n",
    "        out = self.decoder(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72fdf39b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Shows the prediction of the autoencoder without training\n",
    "# Not very good huh? (though theres a small chance it is lol)\n",
    "\n",
    "input_shape = (3,109,89)\n",
    "\n",
    "test_model = Autoencoder(input_shape, 100) # Takes input of celebA image size and encodes it to size 100 and then decodes it\n",
    "test_output = test_model(ex_image)\n",
    "\n",
    "print(\"The original image\")\n",
    "plot_image(ex_image.byte())\n",
    "plt.show()\n",
    "print(\"Your reconstruction\")\n",
    "plot_image(test_output.detach().byte())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9df5e4af",
   "metadata": {},
   "source": [
    "### Loss and Optimizer for Autoencoder (5 points)\n",
    "\n",
    "The loss for our autoencoder is nice and easy since we can just compare the input and output directly -- MSE, MAE or any other loss you'd like would work, though you can try multiple to see how they behave (or make your own if you're a nerd)\n",
    "\n",
    "Likewise we can use can use whichever optimizer we would like from the torch library, using ```torch.optim.SGD()``` or ```torch.optim.Adam()``` would make a lot of sense and should be considered the norm. You are free to try out others though\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6668b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Fill in the loss_function and optimizer below and run this cell to see if they are valid!\n",
    "\n",
    "model = Autoencoder(input_shape, 100)\n",
    "ex_image = ex_image.float().view(-1,3,109,89)                               \n",
    "\n",
    "# TODO fill out your chosen loss_function and optimizer\n",
    "#############################################\n",
    "\n",
    "loss_function = None                           ## You can choose whichever loss function you'd like (MSE is fine if you like it)\n",
    "optimizer = None                               ## You can use Adam for this, which is defined in torch.optim -- look up some API stuff (or HW3 lol))\n",
    "\n",
    "#############################################\n",
    "\n",
    "# This checks that your model, loss and optimizer are valid\n",
    "print(\"BEFORE GRADIENT STEP:\")\n",
    "ex_pred = model(ex_image)\n",
    "ex_label = ex_image\n",
    "\n",
    "\n",
    "optimizer.zero_grad() # Sets the gradient to 0 so that gradients don't stack together\n",
    "\n",
    "ex_loss1 = loss_function(ex_pred, ex_label)\n",
    "print(\"loss\",ex_loss1.item())\n",
    "\n",
    "ex_loss1.backward() # This gets the gradient of the loss function w.r.t all of your model's params\n",
    "\n",
    "print()\n",
    "print(\"AFTER GRADIENT STEP:\")\n",
    "optimizer.step() # This takes the step to train\n",
    "\n",
    "ex_pred = model(ex_image)\n",
    "ex_label = ex_image\n",
    "\n",
    "ex_loss2 = loss_function(ex_pred, ex_label)\n",
    "print(\"loss\",ex_loss2.item())\n",
    "\n",
    "print()\n",
    "print(\"Difference in loss:\", (ex_loss1 - ex_loss2).item())\n",
    "print(\"This should be some positive number to say we reduced loss\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d080f3d8",
   "metadata": {},
   "source": [
    "### Training Loop (10 points)\n",
    "We're ready to train our autoencoder! Complete the ```training()``` function, just like in HW3. If your training is taking a really long time you should either change the batch size above (higher batch_size will train faster in general) or reduce the size of your model (number of parameters) \n",
    "\n",
    "[Hint for reseting the optimizer](https://pytorch.org/docs/stable/generated/torch.optim.Optimizer.zero_grad.html#torch.optim.Optimizer.zero_grad)\n",
    "\n",
    "[Hint for stepping with the optimizer](https://pytorch.org/docs/stable/generated/torch.optim.Optimizer.step.html#torch.optim.Optimizer.step) (You'll have to use .backward() to get the gradient)\n",
    "\n",
    "At this point you should record your training and validation *losses* and *accuracies* **(four lists in total)**. You'll need these values for the written section, where you will be plotting them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0911b2b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def autoencoder_training(model, loss_function, optimizer, train_data, n_epochs, update_interval):\n",
    "\n",
    "    '''\n",
    "    Updates the parameters of the given model using the optimizer of choice to\n",
    "    reduce the given loss_function\n",
    "\n",
    "    This will iterate over the dataloader 'n_epochs' times training on each batch of images\n",
    "    \n",
    "    To get the gradient (which is stored internally in the model) use .backward() from the loss tensor\n",
    "    and to apply it use .step() on the optimizer\n",
    "\n",
    "    In between steps you need to zero the gradient so it can be recalculated -- use .zero_grad for this\n",
    "    '''\n",
    "    \n",
    "    losses = []\n",
    "\n",
    "    for n in range(n_epochs):\n",
    "        for i, image in enumerate(tqdm(train_data)):\n",
    "\n",
    "            image = image.float().view(-1,3,109,89)\n",
    "\n",
    "            # TODO Complete the training loop using the instructions above\n",
    "            # Hint: the above code essentially does one training step\n",
    "\n",
    "            ##############################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            ##############################################################\n",
    "        \n",
    "            if i % update_interval == 0:\n",
    "                losses.append(round(loss.item(), 2)) # This will append your losses for plotting -- please use \"loss\" as the name for your loss\n",
    "        \n",
    "    return model, losses\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f29ea618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plug in your model, loss function, and optimizer \n",
    "# Try out different hyperparameters and different models to see how they perform\n",
    "\n",
    "lr = 1e-4                # The size of the step taken when doing gradient descent\n",
    "batch_size = 128         # The number of images being trained on at once\n",
    "update_interval = 10     # The number of batches trained on before recording loss\n",
    "n_epochs = 1             # The number of times we train through the entire dataset\n",
    "compression_size = 200    # This is the size of the bottleneck (compression point) of the autoencoder\n",
    "\n",
    "input_shape = (3,109,89)\n",
    "\n",
    "dataset = dataset      # The dataset is a pain to load/unload so we want to keep it active\n",
    "\n",
    "model = Autoencoder(input_shape, compression_size) \n",
    "loss_function = nn.MSELoss()                                # Keep this as MSE so that we have a consistent metric we can score on\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)     # You can change this to whatever you'd like\n",
    "\n",
    "trained_model, losses = autoencoder_training(model, loss_function, optimizer, dataset, n_epochs=n_epochs, update_interval=update_interval)\n",
    "\n",
    "plt.plot(np.arange(len(losses)) * batch_size * update_interval, losses)\n",
    "plt.title(\"training curve\")\n",
    "plt.xlabel(\"number of images trained on\")\n",
    "plt.ylabel(\"Reconstruction loss\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# NOTE: It will take a while for this to train (depending on your model)\n",
    "# You can increase the batch size (way up top) or reduce the size of your model if it takes too long\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6fe23e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displays the reconstruction of the (now trained) autoencoder on the same example image\n",
    "# Notice that it worked and we have a better prediction (if your code works)\n",
    "ex_image = dataset[random.randint(0,100)]\n",
    "trained_output = trained_model(ex_image)\n",
    "\n",
    "print(\"original image:\")\n",
    "plot_image(ex_image)\n",
    "plt.show()\n",
    "print(\"your (trained) reconstruction\")\n",
    "plot_image(trained_output.detach().byte())\n",
    "\n",
    "# NOTE: It is very likely that this won't look that good at first\n",
    "# It may even give the same output independent of the input -- rerun this a few times to check\n",
    "# Play with the hyperparameters until you're happy with the output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b7498d",
   "metadata": {},
   "source": [
    "### Probably not great huh ^\n",
    "\n",
    "Let's tune our hyperparameters so that we get something a bit cooler!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a12a2c5",
   "metadata": {},
   "source": [
    "### Testing and HyperParameter Search (10 points)\n",
    "\n",
    "Since the testing loop and training loop are so similar I'm going to go ahead and just give it to you -- but you gotta promise to at least look at the method to see how similar they are! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffc5a564",
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing(model, loss_function, test_data):\n",
    "\n",
    "    '''\n",
    "    This function will test the given model on the given test_data\n",
    "    it will return the accuracy and the test loss (given by loss_function) \n",
    "    '''\n",
    "    \n",
    "    sum_loss = 0\n",
    "\n",
    "    for i, image in enumerate(tqdm(test_data)):\n",
    "\n",
    "        # This is essentially exactly the same as the training loop \n",
    "        # without the, well, training, part\n",
    "        \n",
    "        pred = model(image)\n",
    "        loss = loss_function(pred, image)\n",
    "        sum_loss += loss.item()\n",
    "    \n",
    "    avg_loss = round(sum_loss / len(test_data), 2)\n",
    "\n",
    "    print(\"test loss:\", avg_loss )\n",
    "\n",
    "    return avg_loss\n",
    "\n",
    "def train_and_test(model, loss_function, optimizer, batch_size, update_interval, n_epochs):\n",
    "\n",
    "    '''\n",
    "    This will use your/my methods to create a dataloader, train a gven model, and then test its performance\n",
    "\n",
    "    Again, since I gave this to you for free you have to promise to look at it and try to understand it\n",
    "    '''\n",
    "\n",
    "    train_dataset = dataset[1000//batch_size:]\n",
    "    test_dataset = dataset[:1000//batch_size]\n",
    "\n",
    "    trained_model, losses = training(model, loss_function, optimizer, train_dataset, n_epochs=n_epochs, update_interval=update_interval)\n",
    "\n",
    "    test_loss = testing(trained_model, loss_function, test_dataset)\n",
    "\n",
    "    plt.plot(np.arange(len(losses)) * batch_size * update_interval, losses, color=\"b\", label=\"train loss\")\n",
    "    plt.hlines(test_loss, 0, len(losses) * batch_size * update_interval, color='r', label=\"test loss\")\n",
    "    plt.legend()\n",
    "    plt.title(\"training curve\")\n",
    "    plt.xlabel(\"number of images trained on\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.show()\n",
    "\n",
    "    return trained_model, test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df0a9b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_test_loss = testing(trained_model, loss_function, dataset[:(1000//batch_size)]) # you'll need to change this if you changed batch_size\n",
    "print(avg_test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2795a159",
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO Implement a hyperparameter search of your choice \n",
    "# This could be grid search, random search, or k-fold validation (which is harder)\n",
    "# Use the given train_and_test() function to determine which are the best hyperparameter settings\n",
    "# I'm not going to give any hand-holdy code cause I believe in you! (and NOT because I'm lazy)\n",
    "\n",
    "#######################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#######################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf172a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use your best hyperparameters -- your final test loss should be under 2000\n",
    "\n",
    "lr = 1e-4               # The size of the step taken when doing gradient descent\n",
    "batch_size = 128        # The number of images being trained on at once\n",
    "update_interval = 10    # The number of batches trained on before recording loss\n",
    "n_epochs = 10           # The number of times we train through the entire dataset\n",
    "compression_size = 100  # The output size of the encoder\n",
    "\n",
    "model = Autoencoder(input_shape, compression_size) \n",
    "loss_function = nn.MSELoss()                        \n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr) \n",
    "\n",
    "best_model, _ = train_and_test(model, loss_function, optimizer, batch_size=batch_size, update_interval=update_interval, n_epochs=n_epochs)\n",
    "\n",
    "ex_image = dataset[random.randint(0,100)]\n",
    "trained_output = trained_model(ex_image)\n",
    "\n",
    "print(\"original image:\")\n",
    "plot_image(ex_image)\n",
    "plt.show()\n",
    "print(\"your (BEST) reconstruction\")\n",
    "plot_image(trained_output.detach().byte())\n",
    "\n",
    "# Try to get a reconstruction that you are happy with\n",
    "# It is difficult though so try to set up a big search and go for a hike or something\n",
    "# be warned that google collab sometimes cuts off after some time so be careful!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65978984",
   "metadata": {},
   "source": [
    "## Autoencoder Written Report (10 points)\n",
    "Now, lets take a bit of break from implementing models and do some writing (I know you all love that right?)\n",
    "Fill out your answer to each question in the empty markdown cell below each question.\n",
    "\n",
    "1. What would happen if the compression size of your autoencoder was as large as the input image? Try it and tell me what you found out!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2141e73",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b21fb5e3",
   "metadata": {},
   "source": [
    "2. Was your model able to output any faces that were looking in different directions? Why do you think it would be hard for an autoencoder to learn to output faces with different orientations? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2130ea06",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5dcec8cb",
   "metadata": {},
   "source": [
    "## Generative Adversarial Network (60 points)\n",
    "\n",
    "Now it's time for us to make a Generative Adversarial Network (GAN)! \n",
    "\n",
    "GANs contain a generator that generates an image based on a given dataset, and a discriminator (classifier) to distinguish whether an image is real or generated.\n",
    "\n",
    "GANs are very similar to autoencoders in the sense that we will create two different models, but we will actually train them on different losses!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dbd742e",
   "metadata": {},
   "source": [
    "### The GAN model (20 points)\n",
    "\n",
    "In this part you will create your model for both the Discriminator and Generator. The discriminator will take in an input the size of the images and output a bit which represents either real or fake (the discriminators geuss as to the input is real data or generated)\n",
    "\n",
    "The generator will take in an input of some size (it will end up being noise but this wont come up in the model making part) and then output an \"image\" that is the same size as the real data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff058e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Discriminator(nn.Module):\n",
    "\n",
    "    '''\n",
    "    This will be your discriminator half of you GAN\n",
    "    it will take in something of the shape of an image of a face\n",
    "\n",
    "    it will then return either 0 or 1 depending on whether it\n",
    "    believes the input is from the real distribution or not\n",
    "    '''\n",
    "\n",
    "    def __init__(self, input_shape):\n",
    "        super(Discriminator, self).__init__()\n",
    "\n",
    "        # TODO Initialize your discriminator\n",
    "        # You can linear and conv layers -- as well as anything else you find (don't forget you nonlinearity) \n",
    "        # HINT: if it trains too slow try reducing the dimensions for your linera layers somehow\n",
    "        ##################################### \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        ##################################### \n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # TODO fill out the forward pass of your model\n",
    "        # Don't forget nonlinearities!\n",
    "        ####################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        ####################################\n",
    "\n",
    "        x = nn.Sigmoid()(x) # This sigmoid will squish the outputs between 0 and 1 (you can change this if you'd like but some things may break)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5453ad46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is the performance of the Discriminator (before training) on the example image (which should be 1)\n",
    "# If you rerun this it should change since we are randomly initializing the model\n",
    "discriminator = Discriminator((3,218,178))\n",
    "ex_output = discriminator(ex_image.float())\n",
    "\n",
    "plot_image(ex_image)\n",
    "print(\"Output of the discriminator given this input:\", ex_output[0].detach().numpy()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93e9dbe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    '''\n",
    "    Your model for a generator -- this will take in a vector of noise (sample from gaussians)\n",
    "    of size 'input_size' and output an \"image\" of shape 'output_shape'\n",
    "\n",
    "    You can get this shape however you'd like (you can use torch.reshape() or torch.view() to change from vector to tensor)\n",
    "    You may also use convolution/deconvolution layers to do this too\n",
    "    '''\n",
    "    def __init__(self, input_size, output_shape):\n",
    "        super(Generator, self).__init__()\n",
    "\n",
    "        # TODO complete the initialization of your generator \n",
    "        # Which is a network that goes from input_size to output_shape\n",
    "        # Hint: generally deconvolutions are used, but they are unecessary -- explore for yourself\n",
    "        #############################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #############################################\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # TODO complete the forward pass of your generator model\n",
    "        # We give a sigmoid in the output to contain the output between 0 and 1\n",
    "        # You can change this if you'd like (it may even work better), but remember that you did that\n",
    "\n",
    "        ####################################### \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        ####################################### \n",
    "\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "488d379f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will show the output of our generator before training (it's fine if its all black)\n",
    "test_gen = Generator(128, (3, 109, 89))\n",
    "noise = (torch.rand(1, 128) - 0.5) / 0.5\n",
    "test_output = test_gen(noise)\n",
    "\n",
    "plot_image(test_output.detach().byte())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b44e2b",
   "metadata": {},
   "source": [
    "### This is our generator's attempt at making something before training ^\n",
    "\n",
    "Let's train it to see how it can improve! \n",
    "\n",
    "### Training Loop (20 points)\n",
    "The training for a GAN is fundementally the same for all the other models we train with pytorch (zero grad, output, loss, loss backward, optimizer step). But we are going to do 2 seperate updates in each loop, with different losses!\n",
    "\n",
    "For each loop you will calculate the loss for both the discriminator and generator and then update those models accordingly. Some code is provided to help you out, but not all of it! \n",
    "\n",
    "You should record your *losses* for both the generator and the discriminator **(two lists in total)**. You'll need these values for the written section, where you will be discussing them.\n",
    "\n",
    "Then, use your wisdom from the autoencoder hyperparameter search to find good settings to all the hyperparamers and try your best to get your model to produce a face!\n",
    "\n",
    "[Hint for reseting the optimizer](https://pytorch.org/docs/stable/generated/torch.optim.Optimizer.zero_grad.html#torch.optim.Optimizer.zero_grad)\n",
    "\n",
    "[Hint for stepping with the optimizer](https://pytorch.org/docs/stable/generated/torch.optim.Optimizer.step.html#torch.optim.Optimizer.step) (You'll have to use .backward() to get the gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc04d1ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(generator, discriminator, loss, g_optimizer, d_optimizer, train_dataloader, n_epochs, update_interval, noise_samples):\n",
    "    \n",
    "    g_losses = []\n",
    "    d_losses = []\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        for i, image in enumerate(tqdm(train_dataloader)):\n",
    "\n",
    "            # Training the discriminator\n",
    "            # Real inputs are actual images from the CelebA dataset\n",
    "            # Fake inputs are from the generator\n",
    "            # Real inputs should be classified as 1 and fake as 0\n",
    "            \n",
    "            image = image.float()\n",
    "            \n",
    "            real_classifications = discriminator(image)\n",
    "            real_labels = torch.ones(image.shape[0])\n",
    "\n",
    "            noise = torch.from_numpy((np.random.randn(image.shape[0], noise_samples) - 0.5) / 0.5).float()  ## This is us sampling gaussian noise\n",
    "            fake_inputs = generator(noise)\n",
    "            fake_classifications = discriminator(fake_inputs)\n",
    "            fake_labels = torch.zeros(image.shape[0])\n",
    "\n",
    "            classifications = torch.cat((real_classifications, fake_classifications), 0).reshape(len(real_classifications) + len(fake_classifications))\n",
    "            targets = torch.cat((real_labels, fake_labels), 0)\n",
    "\n",
    "            # TODO Calculate the loss for the discriminator and apply the gradient\n",
    "            # This is the same as a normal training loop!\n",
    "            #######################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            #######################################################\n",
    "            \n",
    "            if i % update_interval == 0:\n",
    "                d_losses.append(round(d_loss.item(), 2))\n",
    "            \n",
    "\n",
    "            # We do a seperate forward pass to update the gradient for the generator since \n",
    "            # Pytorch doesnt like us reusing the same computation graph (it makes one under the hood)\n",
    "            noise = torch.from_numpy((np.random.randn(image.shape[0], noise_samples) - 0.5) / 0.5).float()\n",
    "            fake_inputs = generator(noise)\n",
    "            fake_classifications = discriminator(fake_inputs)\n",
    "            fake_labels = torch.zeros(image.shape[0], 1)\n",
    "\n",
    "            # TODO Calculate the loss for the generator and apply the gradient\n",
    "            # HINT: the loss for the generator is essentially the opposite of the \n",
    "            # discriminators loss but doesnt care about the real examples (they dont go through the generator at all)\n",
    "            #######################################################\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "            ########################################################\n",
    "\n",
    "            if i % update_interval == 0:\n",
    "                g_losses.append(round(g_loss.item(), 2))\n",
    "                \n",
    "    return (generator, discriminator), (g_losses, d_losses) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72274ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 2e-5               # The size of the step taken when doing gradient descent\n",
    "batch_size = 64         # The number of images being trained on at once\n",
    "update_interval = 10    # The number of batches trained on before recording loss\n",
    "n_epochs = 1            # The number of times we train through the entire dataset\n",
    "noise_samples = 128     # The size of the noise input to the Generator\n",
    "\n",
    "loss_function = nn.BCELoss()\n",
    "\n",
    "G_model = Generator(noise_samples, (3,109,89))\n",
    "D_model = Discriminator((3,109,89))\n",
    "G_optimizer = torch.optim.Adam(G_model.parameters(), lr=lr)       # This is an improved version of SGD which decreases the learning rate over time to avoid leaving a minima\n",
    "D_optimizer = torch.optim.Adam(D_model.parameters(), lr=lr)       # This is an improved version of SGD which decreases the learning rate over time to avoid leaving a minima\n",
    "\n",
    "train_dataset = dataset\n",
    "\n",
    "models, losses = training(G_model, D_model, loss_function, G_optimizer, D_optimizer, train_dataset, n_epochs, update_interval, noise_samples)\n",
    "\n",
    "G_model, D_model = models\n",
    "g_losses, d_losses = losses\n",
    "\n",
    "plt.plot(np.arange(len(g_losses)) * batch_size * update_interval, g_losses)\n",
    "plt.title(\"training curve for generator\")\n",
    "plt.xlabel(\"number of images trained on\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.show()\n",
    "\n",
    "plt.plot(np.arange(len(d_losses)) * batch_size * update_interval, d_losses)\n",
    "plt.title(\"training curve for discriminator\")\n",
    "plt.xlabel(\"number of images trained on\")\n",
    "plt.ylabel(\"loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "400d0cf5",
   "metadata": {},
   "source": [
    "Now let's take a look at the generated images coming out of our trained GAN!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f36a6fc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will show the same example as before with the discriminator's new score \n",
    "# 0 is fake and 1 is real -- is it good at discriminating?\n",
    "\n",
    "trained_output = D_model(ex_image.float())\n",
    "\n",
    "plot_image(ex_image)\n",
    "print(\"Output of the discriminator given this input:\", trained_output[0].detach().numpy()[0])\n",
    "plt.show()\n",
    "\n",
    "noise = (torch.rand(1, G_model.input_size) - 0.5) / 0.5\n",
    "trained_gen = G_model(noise)\n",
    "\n",
    "plot_image(trained_gen.detach())\n",
    "\n",
    "trained_output = D_model(trained_gen.float())\n",
    "\n",
    "print(\"Output of the discriminator given this generated input:\", trained_output[0].detach().numpy()[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2e95a9",
   "metadata": {},
   "source": [
    "### GAN hyperparameter Search (10 points)\n",
    "\n",
    "GANs are notoriously hard to train -- generally you have to do a lot of hyper parameter searching to \n",
    "find good settings. Try it out until you get results above that you're happy with it!\n",
    "You're going to have to write a good amount of code for this, but you can base it off of what is above if you want"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aab5d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO Do a hyper parameter search and find the best settings for your model \n",
    "# Again I leave this up to you as to what to do, but I'm gonna warn you that trying them by hand \n",
    "# is probably going to be too slow to work\n",
    "#######################################################\n",
    "\n",
    "\n",
    "#######################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f653ebe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This will show the output of our *best* generator after training\n",
    "noise = (torch.rand(1, 128) - 0.5) / 0.5\n",
    "trained_output = G_model(noise)\n",
    "\n",
    "plot_image(trained_output.detach())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa1a5df",
   "metadata": {},
   "source": [
    "## GAN Written Report (10 points)\n",
    "More writing, yay! Hopefully these questions will make you think!\n",
    "\n",
    "1. Does your trained discriminator learn to correctly discriminate generated examples -- how does yours perform above? What would you guess is the ideal discriminator performance of a trained GAN? Why? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c0812b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9cb6ced9",
   "metadata": {},
   "source": [
    "2. Sometimes our generator can produce images that dont look at all like faces (to us) but still fools our discriminator. We can these exmaples *adverserial examples* for our disriminator. Why might our generator produce images like this instead of faces? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06016fe",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "92e3cdfe",
   "metadata": {},
   "source": [
    "## BONUSES (5 points each)\n",
    "\n",
    "These are some extra questions that require more code or are just downright hard -- if you're interested in this stuff it could be fun though!\n",
    "\n",
    "1. Write some code to augment your input to your autoencoder to either be black and white or occluded (part of the image missing) and train it to output the original unaugmented image. Show the results of your model after training below (you can edit whatever code you like to make this work)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cba32ab",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7949d50d",
   "metadata": {},
   "source": [
    "2. After getting your generator able to generate faces (if you are able), show what it generates for some particular noise and then iteratively change the noise by a little bit. Does the generator produce faces that are similar for similar inputs?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f76b37a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
