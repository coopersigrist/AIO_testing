{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0a56ccb",
   "metadata": {},
   "source": [
    "# Backprop and Gradient Descent Test\n",
    "MLPs, SGD, MSE, all of the 3 letter acronyms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01db7cc0",
   "metadata": {},
   "source": [
    "## Section 1 (15 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50b342d",
   "metadata": {},
   "source": [
    "In this section you will derivive the gradient of an MLP with Sigmoid nonlinearities for use in gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08c1a139",
   "metadata": {},
   "source": [
    "1. For the following nonlinearity, calculate the derivative of it w.r.t (with respect to) its input -- this was on the previous test, but now you must simplify (hint: $\\sigma(X)$ will appear in your solution):\n",
    "\n",
    "\\begin{align*}\n",
    "    σ(X) = \\frac{1}{1+e^{-X}}\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "    \\frac{∂σ}{∂X}=\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d86c99d",
   "metadata": {},
   "source": [
    "2. Imagining that we’re using this nonlinearity after a single layer of neural network – calculate the derivative of the output w.r.t the weights.\n",
    "\n",
    "\\begin{align*}\n",
    "    A = X \\cdot W + b \\quad \\text{  and  } \\quad \\hat{y} = σ(A) = \\frac{1}{1+e^{-A}}\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "    \\frac{∂\\hat{y}}{∂W}=\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba9a19c",
   "metadata": {},
   "source": [
    "3. Now consider finding the derivative of a model with an additional layer like so:\n",
    "\n",
    "\\begin{align*}\n",
    "    \\hat{y} = (σ(X\\cdot W_{0}+b_{0})) \\cdot W_{1} + b_{1}\n",
    "\\end{align*}\n",
    "\n",
    "\\begin{align*}\n",
    "    \\frac{∂\\hat{y}}{∂W_0}=\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27f6f46f",
   "metadata": {},
   "source": [
    "4. Describe and implement a modification that can be made to X and W, to X' and W',such that:\n",
    "\n",
    "\\begin{align*}\n",
    "    X \\cdot W + b = X' \\cdot W'\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e94ca4e",
   "metadata": {},
   "source": [
    "Describe here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d12afc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def modify_x_w(x, w, b):\n",
    "    '''\n",
    "    You may assume that x,w, and b are all given as np arrays\n",
    "    '''\n",
    "    X_new = None\n",
    "    W_new = None\n",
    "\n",
    "    return X_new, W_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3834d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for your implementation\n",
    "import numpy as np\n",
    "\n",
    "# test vectors:\n",
    "X = np.array([1,2,3,4])\n",
    "W = np.ones((4,2))\n",
    "b = np.ones(2)\n",
    "\n",
    "X_new, W_new = modify_x_w(X,W,b)\n",
    "\n",
    "print(\"original (vector): \" + str((X @ W) + b))\n",
    "print(\"modified (vector): \" + str((X_new @ W_new)))\n",
    "\n",
    "# test matrices:\n",
    "X = np.array(([1,2,3], [4,5,6]))\n",
    "W = np.array(([1,2,3,10], [4,5,6,10], [7,8,9,10]))\n",
    "b = np.array([[5,4,6,9]])\n",
    "\n",
    "X_new, W_new = modify_x_w(X,W,b)\n",
    "\n",
    "print(\"original (matrix): \" + str((X @ W) + b))\n",
    "print(\"modified (matrix): \" + str((X_new @ W_new)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "842308ac",
   "metadata": {},
   "source": [
    "## Section 2 (15 min)\n",
    "In this section you will create a dataset and plot it"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd0581bb",
   "metadata": {},
   "source": [
    "1. Create a function to sample n points from the following distribution:\n",
    "\n",
    "\\begin{align*}\n",
    "X \\sim U[-1, 1], \\quad \\quad\n",
    "Y \\sim U[-1, 1], \\quad \\quad          \n",
    "Err \\sim N(0, 0.5) \\\\ \\\\\n",
    "Z = X^2 - Y^2 + 1.2 + Err\n",
    "\\end{align*}\n",
    "\n",
    "The function should return an n x 3 matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e318224",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_points(n):\n",
    "    '''\n",
    "    Sample from the above function n times and return the resultant n x 3 matrix as an np array\n",
    "    Ensure that the order of the colomns are X, Y, Z (i.e. points[:][2] should be an nx1 col of just the Z values)\n",
    "    '''\n",
    "    points = np.ones((n, 3))\n",
    "\n",
    "    return points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11aa1742",
   "metadata": {},
   "source": [
    "2. Now plot these points as a 3d scatter plot using matplotlib, x, y, and z should correspond to their respective axes. The points should be colored (*) according to their z value (blue is low, red is high) and should have opacity 0.5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e0bee5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_3d_scatter(points):\n",
    "    \n",
    "    plt.show()\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e16f478a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test for sampling and plotting:\n",
    "\n",
    "points = sample_points(100)\n",
    "plot_3d_scatter(points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "767eb8cb",
   "metadata": {},
   "source": [
    "3. Next create a function that will create a train and test dataset of size 100 and 20 respectively. Then plot both on the same 3d scatter in different shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2572ec29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_train_and_test(train_size=100, test_size=20):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4c19c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_train_and_test(train_data, test_data):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ed0967",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your code\n",
    "train_data, test_data = create_train_and_test()\n",
    "plot_train_and_test(train_data, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c706eb",
   "metadata": {},
   "source": [
    "# Section 3 (10 min)\n",
    "Now we will initilize a model to train on this data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aba27a56",
   "metadata": {},
   "source": [
    "1. First we intialize weights, create a function that will initialize an IN x OUT size weight matrix with each value sampled from a normal distribution with mean 1 and std 0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "78f99b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def init_weight_matrix(In=2, Out=1):\n",
    "\n",
    "    weight_matrix = np.ones((In, Out)) # Not correct, just an example\n",
    "\n",
    "    return weight_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4781a59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test this\n",
    "\n",
    "test_mat = init_weight_matrix(5, 10)\n",
    "print(test_mat)\n",
    "print(test_mat.shape)\n",
    "print(np.mean(test_mat))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "684f5180",
   "metadata": {},
   "source": [
    "2. Now we want to be able to initialize an MLP. We will be given a list of output sizes for our matrices. Since the data we are using as input is 2d the first matrix will always have an input size of the first element of the list but subsequent layers will have input size equal to the previous layers output size. e.g. a size list [2,5,10,1] should produce a 2x5, 5x10, and 10x1 sized matrices.\n",
    "\n",
    "The model should be given as a dictionary with the key 'W+str(n)' corresponding to the n-th weight matrix of the MLP."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a60c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_mlp(output_sizes=[5,1]):\n",
    "    model = {}\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccd5d457",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "\n",
    "test_mlp = init_mlp([10,5,2,1])\n",
    "\n",
    "for layer in test_mlp:\n",
    "    print('layer : ' + layer)\n",
    "    print(test_mlp[layer].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7dc4d1",
   "metadata": {},
   "source": [
    "# Section 4 (10 min)\n",
    "Now we will complete the forward pass and loss calculation of our model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b97ebc5",
   "metadata": {},
   "source": [
    "1. First let's complete the forward pass of the sigmoid function (we will use this as our nonlinearity)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22694f4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_forward(x):\n",
    "    '''\n",
    "    returns the output of the sigmoid non-linearity performed on input X (elementwise)\n",
    "    '''\n",
    "\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe8fd18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "example_inputs = np.arange(-5,5,0.25)\n",
    "your_output = sigmoid_forward(example_inputs)\n",
    "\n",
    "plt.plot(example_inputs, your_output)\n",
    "plt.title(\"Your sigmoid function\")\n",
    "plt.xlabel(\"Input\")\n",
    "plt.ylabel(\"Output\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a8a2666",
   "metadata": {},
   "source": [
    "2. Now we will do the full forward pass of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0be4c6ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlp_forward(my_mlp, x):\n",
    "    '''\n",
    "    To complete this we must do the following steps:\n",
    "    1. Alter x such that the bias term is implicit in the matrix operations (via your solution to 1.4)\n",
    "    2. Repeatedly perform matrix multiplication via our mlp weights, with a nonlinearity (sigmoid forward) after all but the final\n",
    "    3. Each layer's output (after sigmoid) must be stored in a cache as 'A+str(l)' for the L-th layers output, with 'A0' as X\n",
    "    '''\n",
    "\n",
    "    cache = {'A0': x}\n",
    "    output = None\n",
    "\n",
    "    return cache, output\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80270643",
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing your code in tandem\n",
    "\n",
    "test_mlp = init_mlp([2,5,1]) # change the input size if necessary for your implicit bias\n",
    "test_data = sample_points(100)\n",
    "test_input = test_data[:][:2]\n",
    "test_label = test_data[:][2]\n",
    "\n",
    "# You may make modifications to this if necessary to test your code -- thought I would like this as a quick way to check correctness\n",
    "\n",
    "test_cache, test_output = mlp_forward(test_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8aa87cb",
   "metadata": {},
   "source": [
    "# Section 5 (*) (25 min)\n",
    "Now we will finally do our loss, backprop and gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a364da6",
   "metadata": {},
   "source": [
    "1. First let's create our loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f616ed22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(label, pred):\n",
    "    '''\n",
    "    return the MSE loss of some pred against some true label -- can be one line\n",
    "    '''\n",
    "\n",
    "\n",
    "    return "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a3a2ea",
   "metadata": {},
   "source": [
    "2. Second lets get the gradient of our model via backprop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ed9030",
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop(my_mlp, cache, pred):\n",
    "    '''\n",
    "    Using the cache, find the derivatives of loss with respect to each weight matrix,\n",
    "    store them each in a cache with the key 'dW+str(l)' corresponding to the value of the derivative of Weight layer l\n",
    "    '''\n",
    "\n",
    "    dcache = {}\n",
    "\n",
    "    return dcache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21f2403f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test that the dimensions line up\n",
    "\n",
    "test_dcache = backprop(test_mlp, test_cache)\n",
    "\n",
    "for derivative in test_dcache:\n",
    "    print(derivative + ' shape: ' + test_dcache[derivative].shape)\n",
    "\n",
    "# Should have the same shapes as the MLP -- here 2x5, 5x1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910b9ca8",
   "metadata": {},
   "source": [
    "3. Now we can finally implement gradient descent. We will allow a choice of the number of iterations and the learning rate as inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec2428cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grad_descent(data, my_mlp, iterations, learning_rate):\n",
    "    '''\n",
    "    This will repeatedly calculate the output of the model and then apply a step of gradient descent.\n",
    "    A list of the loss after each iteration (and also the loss before any iterations) should be returned\n",
    "    along with the final model.\n",
    "    '''\n",
    "\n",
    "    losses = []\n",
    "\n",
    "    return losses, my_mlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9c7ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test grad descent\n",
    "\n",
    "test_losses, test_final_mlp = grad_descent(test_data, test_mlp, iterations=10, learning_rate=0.0001) # You may need to tune learning rate\n",
    "\n",
    "plt.plot(np.arange(len(test_losses)), test_losses)\n",
    "plt.title(\"Learning Curve\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67ff5224",
   "metadata": {},
   "source": [
    "4. Now we plot our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af945c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_data_and_pred(my_mlp, train_points, test_points):\n",
    "    '''\n",
    "    This should plot the data as a 3d scatter and the prediction of the model (i.e. the forward)\n",
    "    as a 3d surface (that is it should be across all possible values of x and y).\n",
    "     \n",
    "    Make sure that both the surface and the points are distinguishable\n",
    "    '''\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d9122b2",
   "metadata": {},
   "source": [
    "# Section 6 (**) Hyperparameter Tuning (20 min)\n",
    "Now we want to find a good MLP for a problem\n",
    "\n",
    "Note on this section -- if any of the test data is used at any point to influence the choice of hyperparameters then your solution will be useless. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "966ad594",
   "metadata": {},
   "source": [
    "1. First, let us observe the behavior of a one-layer vs two layer network on the above problem. Plot the learning curve (loss vs iteration) of a single-layer and two-layer network trained via your gradient descent. Then plot the surfaces of each compared to the training and testing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be9b0524",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "94705f2c",
   "metadata": {},
   "source": [
    "2. Now, train each, but reserve 20% of the *training data* and evaluate each model on that portion each iteration without training on it. Add these \"validation\" plots to the learning curve plot (each line of a given model should be the same color but the validation line should be dashed)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909292db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c9c3abbe",
   "metadata": {},
   "source": [
    "3. Design and code a procedure to choose the best hyperparameters (i.e. the arcitecture (e.g. layers/weight mat sizes, learning rate, loss func)) to fit to this data. You may only evaluate on the test data (or plot it) once you have chosen a final model. Your performance on this data is considered your final score here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86c82764",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
