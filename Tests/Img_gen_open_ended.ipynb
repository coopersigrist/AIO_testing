{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "83c165fa",
   "metadata": {},
   "source": [
    "# Image Generation Followup\n",
    "This will be an open-ended assignment on image generation -- you will implement a variational autoencoder and take on some challenging problems in image generation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c59caa5",
   "metadata": {},
   "source": [
    "### 0. Dataset and libraries\n",
    "We're going to use MNIST for this portion of the assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf6861a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np     \n",
    "\n",
    "import torch           \n",
    "import torch.nn as nn   # |  This is just shortening the name of this module since we're gonna use it a lot -- this is the one that has neural network objects (nn.modules)\n",
    "import torchvision      # |  This is for importing the vision datasets we'll use\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, Subset # | These are particular objects that we use to load our data (and shuffle it and whatnot) we'll talk more about these later\n",
    "import torchvision.transforms as tt # | Allows us to transform our data while we load it (or after) such as rotating, flipping, ocluding, etc. \n",
    "from torchvision.datasets import ImageFolder # | ^^ less important for you\n",
    "\n",
    "\n",
    "import torch.nn.functional as F # | This is for functional / in-place operations for example if I wanted to do a sigmoid operation, but not as a neural net object (though I can still update through it)\n",
    "\n",
    "\n",
    "\n",
    "from torchvision.utils import make_grid  # |   Utility stuff for plotting\n",
    "import matplotlib.pyplot as plt          # |  <- I use this one a lot for plotting, seaborn is a good alternative\n",
    "from matplotlib.image import imread      # |  it reads images... (png -> usable input (like a numpy array for ex))\n",
    "import os\n",
    "import random\n",
    "from tqdm import tqdm  # | This one is a cute one for making a loading bar, I like it and we'll use it here\n",
    "\n",
    "def load_mnist(batch_size=32, train=True):\n",
    "\n",
    "    '''\n",
    "    Using the dataset and dataloader classes you should be able to make an MNIST set and loader\n",
    "    the loader should use the 'batch_size' argument and the dataset should use'train'\n",
    "\n",
    "    Also, the 'ToTensor' transform is given, you should set the transform of the dataset to just this\n",
    "    '''\n",
    "\n",
    "    to_tensor_transform = torchvision.transforms.ToTensor()\n",
    "    dataset = torchvision.datasets.MNIST('../dataset/', train=train, download=True, transform=to_tensor_transform)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    return dataset, dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b620fb",
   "metadata": {},
   "source": [
    "### 1. Implement a Variational Autoencoder\n",
    "You should define two different modules: Encoder and Decoder. Using these create a training loop for a variational autoencoder. Architecture, loss, hyperparmeter settings, and training procedure are all up to you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5f7228",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b97d5441",
   "metadata": {},
   "source": [
    "### 2.  Generation with a VAE\n",
    "Now you'll use your trained Decoder to generate images -- It should take as input random noise and produce generated examples similar to MNIST. You may need to alter your loss function to force the output of the encoder towards standard gaussian."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aff8bbb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5142c34b",
   "metadata": {},
   "source": [
    "### Challenge 1. Generate specific classes\n",
    "Alter your training / generation procedures such that you can denote a particular class to generate from. You should still be able to generate from the full distribution as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c60841",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4d0eaf32",
   "metadata": {},
   "source": [
    "### Challenge 2: Encoding invariance\n",
    "Train your model such that inputs which are rotations/ reflections/ or shifted will produce encodings that are close. Compare the generated images from the model trained with this additional restirction to your original and discuss."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9efaf712",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "defa679c",
   "metadata": {},
   "source": [
    "### Challenge 3: Interpolation\n",
    "The goal of this challenge is to create a series of images interpolating between 2 arbitrary inputs from the original dataset. Use your autoencoder to produce something like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "048506c4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3efbc69f",
   "metadata": {},
   "source": [
    "### Challenge 4: Color data\n",
    "Now train using the Cifar 10 dataset and produce high-quality, color images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dc73e03",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
