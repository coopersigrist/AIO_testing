{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f90ddd03",
   "metadata": {},
   "source": [
    "# Pytorch and Classification Test\n",
    "Finally some real Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2f7969",
   "metadata": {},
   "source": [
    "## Section 1: Dataset and Dataloaders (5 min)\n",
    "In this section you will implement a torchvision Dataset and Dataload for the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6a705fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np     # <----- Our old friend\n",
    "\n",
    "import torch            # |  Our new best friends -- this is the main pytroch library\n",
    "import torch.nn as nn   # |  This is just shortening the name of this module since we're gonna use it a lot -- this is the one that has neural network objects (nn.modules)\n",
    "import torchvision      # |  This is for importing the vision datasets we'll use\n",
    "from torch.utils.data import Dataset, DataLoader, random_split, Subset # | These are particular objects that we use to load our data (and shuffle it and whatnot) we'll talk more about these later\n",
    "import torchvision.transforms as tt # | Allows us to transform our data while we load it (or after) such as rotating, flipping, ocluding, etc. \n",
    "\n",
    "import torch.nn.functional as F # | This is for functional / in-place operations for example if I wanted to do a sigmoid operation, but not as a neural net object (though I can still update through it)\n",
    "\n",
    "\n",
    "from torchvision.utils import make_grid  # |   Utility stuff for plotting\n",
    "import matplotlib.pyplot as plt          # |  <- I use this one a lot for plotting, seaborn is a good alternative\n",
    "from matplotlib.image import imread      # |  it reads images... (png -> usable input (like a numpy array for ex))\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c19a45d",
   "metadata": {},
   "source": [
    "## MNIST\n",
    "You remeber our good friend MNIST? Well it's about time that you two get acquainted. MNIST is a dataset of 60,000 training and 10,000 testing images of handwritten digits, with (human done) labels of which digit is written. You can find the MNIST official website [here](http://yann.lecun.com/exdb/mnist/) and a description a little less 80's [here](https://deepai.org/dataset/mnist)\n",
    "\n",
    "Notably this dataset has no colors -- and because of how (relatively) simple this dataset is we consider this to be the \"hello world\" of vision datasets.\n",
    "\n",
    "Here's what the dataset looks like to us humans:\n",
    "\n",
    "<center><img src=\"https://external-content.duckduckgo.com/iu/?u=https%3A%2F%2Ftheanets.readthedocs.io%2Fen%2Fstable%2F_images%2Fmnist-digits-small.png&f=1&nofb=1\" width=\"350\" height=\"250\" /><center>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871425d8",
   "metadata": {},
   "source": [
    "In this assignment you will be loading your own data set in order to practice working with *Datasets* in PyTorch. To iterate over your data, PyTorch has two very helpful mechanisms: ```Dataset``` and ```Dataloader```. A ```Dataset``` is a type of PyTorch class which makes it easier to access your data. \n",
    "\n",
    "Here's a reference API that you should 100% use for [Datasets and Dataloaders](https://pytorch.org/vision/0.8/datasets.html#mnist)\n",
    "\n",
    "You can create an object of a ```Dataset()``` class, and then access the size of the data set as ```len(dataset)```.  You can access the actual data at any given index by calling ```dataset[index]```. \n",
    "\n",
    "You can also apply *transformations* to the dataset, which are created by calling some predefined functions in PyTorch (or Torchvision) called [transforms](https://pytorch.org/vision/stable/transforms.html). For this data and HW the only transform you need to use is ```ToTensor()``` which will give us our data as a Tensor (a generalization of scalar, vector, and matrix with arbitrary dimesnions), which will allow us to do gradient descent with Pytorch\n",
    "\n",
    "Once you create a ```Dataset``` class with your needed transformations, you can feed it into a ```Dataloader```. A ```Dataloader``` is an iterable over the ```Dataset```, which is just a fancy term for a helpful Python class that can run loops over your data without much code. For example, if your ```Dataset``` returns a (sample, target) pair, then you can iterate over the ```Dataloader``` as:\n",
    "\n",
    "```\n",
    "train_ds = MyDatasetClass()\n",
    "train_dl = Dataloader(train_dl, batch_size)\n",
    "for input, output in train_dl:\n",
    "    ...\n",
    "```\n",
    "Remember that the sizes of ```input``` and ```output``` are specified by the ```batch_size``` that you selected earlier!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d18ed3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist(batch_size=32, train=True):\n",
    "\n",
    "    '''\n",
    "    Using the dataset and dataloader classes you should be able to make an MNIST set and loader\n",
    "    the loader should use the 'batch_size' argument and the dataset should use'train'\n",
    "\n",
    "    Also, the 'ToTensor' transform is given, you should set the transform of the dataset to just this\n",
    "    '''\n",
    "\n",
    "    to_tensor_transform = torchvision.transforms.ToTensor()\n",
    "    # TODO create a dataset and then dataloader object for MNIST using\n",
    "    # the torchvision library\n",
    "\n",
    "    #############################################\n",
    "    \n",
    "    dataset = None\n",
    "    dataloader = None\n",
    "\n",
    "    ##############################################\n",
    "\n",
    "    return dataset, dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5729384",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_image_and_label(image, label):\n",
    "    \n",
    "    '''\n",
    "    Takes in an image and label and shows them using matplotlib \n",
    "    this is used to visualize the data and also the outputs of our network\n",
    "    '''\n",
    "\n",
    "    plt.imshow(image)\n",
    "    if type(label) is not int:\n",
    "        _,predicted = torch.max(label,1)\n",
    "        plt.title(\"Best label = \" + str(predicted.item()) + \", with Score: \" + str(round(label[0][predicted].item() * 100,2)))\n",
    "    else:\n",
    "        plt.title(\"Label = \" + str(label))\n",
    "    plt.show()\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e9f293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST\n",
    "\n",
    "train_dataset, train_dataloader = load_mnist(batch_size=1, train=True)\n",
    "ex_image, ex_label = train_dataset[random.randint(0,1000)]\n",
    "plot_image_and_label(ex_image.reshape(28,28), ex_label)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c67f22",
   "metadata": {},
   "source": [
    "## Section 2: Torch Modules (10 min)\n",
    "\n",
    "Pytorch is a library that will allow you to define neural network objects (nn.modules) for your entire network of whatever operations you want, select a loss function, and then ...\n",
    " automatically calculate the gradient for you\n",
    "\n",
    "Pytorch is built off of modules (called ```nn.Module```) which consist of 2 parts: The initialization (defined in ```__init__()``` -- note that this the python convention for initalizing classes) and the forward pass (defined aptly as ```forward()```)\n",
    "\n",
    "What is magical about Pytorch is that you simply define these two things and then the gradient can be found *auotmatically*. So all that grueling code you wrote last time... totally unnecessary now. It still helps you in the long run though -- I promise. \n",
    "\n",
    "Documentation for a pytorch module can be found [here](https://pytorch.org/docs/stable/generated/torch.nn.Module.html)\n",
    "\n",
    "In this section we will create a Mulit-Layer perceptron using "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ab071d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyMLP(nn.Module):\n",
    "\n",
    "    '''\n",
    "    in init you should initialize your model\n",
    "    in forward you just need to use the layers initialized in init to get the output of your model\n",
    "\n",
    "    The input 'sizes' is a list of the layer sizes, with sizes[0] being the input size and sizes[-1] being the output size.\n",
    "    This model will only use linear layers so you must flatten the input appropriately.\n",
    "    '''\n",
    "\n",
    "    def __init__(self, sizes=[784, 20, 10]):\n",
    "        super(MyMLP, self).__init__() \n",
    "\n",
    "        # TODO initalize your model here \n",
    "        #################################\n",
    "\n",
    "        pass\n",
    "\n",
    "        #################################\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # TODO perform the forward pass of you model \n",
    "        # use the module you initialized above\n",
    "        #################################\n",
    "\n",
    "        out = None\n",
    "\n",
    "        #################################\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0d31d0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For testing your model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ffb5357",
   "metadata": {},
   "source": [
    "## Section 3 : Classification (15 min)\n",
    "In this section we will implement the Cross-Entropy loss and create a training loop to minimize it using our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5803d3c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO your implementation of cross entropy -- i.e. A softmax over input scores and then negative log loss on those probabilities and the true probs.\n",
    "def cross_entropy(scores, true_probs):\n",
    "\n",
    "    loss = None # Must return a torch Tensor (i.e. you must be able to differentiate through this)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6f9eddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare to torch's implementation\n",
    "\n",
    "x = [9,5,4,2,1]\n",
    "y = [0.9, 0.01, 0.09, 0, 0]\n",
    "\n",
    "ce_loss = torch.nn.CrossEntropyLoss()\n",
    "print(\"Pytroch CE:\",ce_loss(x,y))\n",
    "print(\"Your CE:\", cross_entropy(x,y))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff8cd093",
   "metadata": {},
   "source": [
    "#### Training loop\n",
    "Now you are finally ready to train your neural network! Complete your ```training()``` function. You can iterate over your data for 30 epochs (epochs = number of times you iterate over all your data) to begin with. In order to write your training loop, the following general structure can be followed:\n",
    "```\n",
    "# initialize loss_function and optimizer\n",
    "model = MyMLP()\n",
    "for iteration in range(epochs):\n",
    "    for input, output in train_dl:\n",
    "        reset optimizer gradients \n",
    "        my_output = model(input)\n",
    "        loss = loss_function(my_output, loss)\n",
    "        step over gradients using optimizer\n",
    "```\n",
    "\n",
    "\n",
    "[Hint for reseting the optimizer](https://pytorch.org/docs/stable/generated/torch.optim.Optimizer.zero_grad.html#torch.optim.Optimizer.zero_grad)\n",
    "\n",
    "[Hint for stepping with the optimizer](https://pytorch.org/docs/stable/generated/torch.optim.Optimizer.step.html#torch.optim.Optimizer.step) (You'll have to use .backward() to get the gradient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0463278b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(lr, epochs, train_data, model):\n",
    "    '''\n",
    "    returns a trained model, as well as a list of training losses and a list of training accuracies -- each should have 1 element per epoch.\n",
    "    '''\n",
    "\n",
    "    return trained_model, train_losses, train_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "267396ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot your training curve here -- both loss and accuracy should be reported. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c57ee99d",
   "metadata": {},
   "source": [
    "## Section 4 : Open-Ended (30 min per)\n",
    "Here I give you 4 open ended challenges. In the live session you may choose one to attempt in the 30 minutes. Afterward I expect that you will complete at least 2 of them. </br> (Additional completed challenges will be noted and will award towards a letter of recommendation.)\n",
    "</br></br>\n",
    "You may use any technique that is available in the given libraries to do each of these."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c31cf18f",
   "metadata": {},
   "source": [
    "### Challenge 1 (Compute Restricted): Oh No! You only have 1kb of RAM but you need to be able to read these digits! You must create a model that uses fewer than 1000 parameters but achieves over 80% training accuracy and 50% test accuracy to pass this challenge. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df6ed7e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "71ce4504",
   "metadata": {},
   "source": [
    "### Challenge 2 (Robustness): Create a model and prove that it is robust to Partial occlution, rotation, and reflection transformations. I do not have a specific number for this one, but it will be up to you to prove through testing or mathematics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e8bf8aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d32da089",
   "metadata": {},
   "source": [
    "### Challenge 3 (Oops! all inputs): Unsupervised learning (no labels) -- traing a model without any labels such that it can achieve >=90% test accuracy. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a075905f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4a0e0880",
   "metadata": {},
   "source": [
    "### Challenge 4 (Black hat beginner): Using any model which you have trained to >=95% train accuracy create 10 examples which are changed from previously correctly-predicted data but are now misclassified. These inputs should be shown side by side (they should not look different). Whoever is able to do this with the smallest magnitude changes will also get special notice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ea7dde",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
