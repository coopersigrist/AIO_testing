{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "203c40d3",
   "metadata": {},
   "source": [
    "# Basis Expansion and Regularization\n",
    "Humans still have to do work too."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71a6f22a",
   "metadata": {},
   "source": [
    "## Section 1 : Polynomial Curve Fitting (20 min)\n",
    "In this section you will implement a polynomial curve fitting algorithm using gradient descent and explore the bias-variance tradeoff."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4102d804",
   "metadata": {},
   "source": [
    "1. First, let's get some data. Create a function to sample n points from the following distribution:\n",
    "$$\\begin{align*}\n",
    "X \\sim U[0,5] \\quad E = N(0, 4) \\\\\n",
    "Y = 3x^2 + 2x -3 + E\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ad10a4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_polynomial(n):\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "469016ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_points(points):\n",
    "    '''\n",
    "    plot the points as well as the polynomial itself\n",
    "    points should be stars and the polynomial should be dashed with alpha=0.4\n",
    "    '''\n",
    "\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a934451",
   "metadata": {},
   "source": [
    "2. Now, let's define our model. Consider our model to be, for some integer n:\n",
    "$$\\begin{align*}\n",
    "\\hat{y} = w_nx^n + w_{n-1}x^{n-1} + ... w_1x + w_0\n",
    "\\end{align*}$$\n",
    "\n",
    " Write a fuction that will take an input, n, and initialize an approporiate model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f704eeb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_poly_model(n):\n",
    "    '''\n",
    "    This should produce a model (as weights stored in a numpy array)\n",
    "    with the above definition.\n",
    "\n",
    "    All weights should be initilized to one\n",
    "    '''\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8908da36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_points_and_models(train_data, test_data, models):\n",
    "    '''\n",
    "    Modify your above code to add the ability to plot models alongside\n",
    "    the data (now training and testing data) and the true polynomial.\n",
    "\n",
    "    Train data should be blue and test data should be red. \n",
    "\n",
    "    The models are given as a list of tuples with:\n",
    "    [0] : numpy array (an n degree model is an n long np array)\n",
    "    [1] : string to label the model in the plot\n",
    "    '''\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c48c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test of your code so far\n",
    "\n",
    "zero_deg = init_poly_model(0)\n",
    "two_deg = init_poly_model(2)\n",
    "five_deg = init_poly_model(5)\n",
    "\n",
    "train_points = sample_polynomial(5)\n",
    "test_points = sample_polynomial(5)  \n",
    "plot_points_and_models(train_points, test_points, [(zero_deg, \"constant model untrained\"), (five_deg, \"5 degree model untrained\")])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2877a6b3",
   "metadata": {},
   "source": [
    "3. Now we train. Implement gradient descent on your polynomial, you may choose the values of your hyperparameters however you like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6c9c75",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(iterations, learning_rate, model, train_data, test_data):\n",
    "    '''\n",
    "    Implement gradient descent using LSE as loss on your polynomial model\n",
    "    in each iteration of training you should record both the train loss and test loss\n",
    "    All should be returned as numpy arrays.\n",
    "\n",
    "    You may create helper functions for this implementation, such as one to calculate the gradient.\n",
    "    '''\n",
    "\n",
    "    return final_model, train_losses, test_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5b57806",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing -- you may need to tune learning rate\n",
    "\n",
    "trained_two_degree = gradient_descent(10, 0.0001, two_deg, train_points, test_points)\n",
    "trained_five_degree = gradient_descent(10, 0.0001, five_deg, train_points, test_points)\n",
    "\n",
    "plot_points_and_models(train_points, test_points, [(trained_two_degree, \"Two degree model trained\"), (trained_five_degree, \"five degree model trained\")])\n",
    "\n",
    "'''\n",
    "TODO plot the learning curves (train and test) as well\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add6e502",
   "metadata": {},
   "source": [
    "## Section 2 : Smarter Polynomial Curve Fitting (20 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bd12f4",
   "metadata": {},
   "source": [
    "1. First we will define a polynomial basis expansion. That is, given some degree d we will create a new input dataset, $\\phi(x)$, like so:\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\phi(x) = [x^d, x^{d-1}, ..., x^{1}, 1]\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d213413",
   "metadata": {},
   "outputs": [],
   "source": [
    "def poly_basis(train_data, d):\n",
    "    '''\n",
    "    Returns a numpy array which is a basis expansion of each element of x, this should be an n x (d+1) matrix output.\n",
    "    Note that x^0 = 1 is included in the basis which allows us to remove bias from a linear layer.\n",
    "\n",
    "    Reminder train_data[:][0] = x, train_data[:][1] = y\n",
    "    '''\n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a42448d",
   "metadata": {},
   "source": [
    "2. Now we fit it. Recall the optimnal linear fit, $w^*$, given LSE from the previous test:\n",
    "$$\\begin{align*}\n",
    "w^* = (X^TX)^{-1} \\cdot (X^Ty)\n",
    "\\end{align*}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498affc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def poly_fit(train_data, test_data, degree):\n",
    "    '''\n",
    "    Create a d-degree fit for data input x and output y.\n",
    "    You must use basis expansion and then use the closed-form linear fit above to achieve this\n",
    "\n",
    "    Your solution (W*) should be returned as an d+1 long np array.\n",
    "\n",
    "    You should also return both the training loss and testing loss -- since there are no iterations these will just be scalars\n",
    "    '''\n",
    "\n",
    "    return opt_w, train_loss, test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdbbcb2",
   "metadata": {},
   "source": [
    "3. Plot your new polynomial fits and answer the following about them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "840e357d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "\n",
    "two_deg  = poly_fit(train_points, test_points, 2) # two_deg is a tuple of the returns of poly_fit\n",
    "five_deg = poly_fit(train_points, test_points, 5) # Same with five_deg\n",
    "\n",
    "print(\"Losses of two deg:\")\n",
    "print(\"Train:\", two_deg[1], \"      |     Test:\", two_deg[2])\n",
    "print(\"--------------------------------------------------------\")\n",
    "print(\"Losses of five deg:\")\n",
    "print(\"Train:\", five_deg[1], \"      |     Test:\", five_deg[2])\n",
    "\n",
    "plot_points_and_models(train_points, test_points, [two_deg[0], five_deg[0]])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a981b143",
   "metadata": {},
   "source": [
    "a. What is the expected optimal training loss (LSE in this case) of an d-degree polynomial trained on n data points with d >= n? Why do we not expect the same for the testing loss?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a465e31",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "18f399bb",
   "metadata": {},
   "source": [
    "b. This gap between train and test loss signifies *Overfitting*, suggest two possible approaches to creating a fit to polynomial data that is not overfit, explain why each would reduce overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac193b09",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8a916470",
   "metadata": {},
   "source": [
    "## Section 3 : Regularization and Hyperparameter Tuning (20 min)\n",
    "For this section recall the following loss function and optimal weights:\n",
    "\n",
    "$$\\begin{align*}\n",
    " \\hat{y} &= X \\cdot W\\\\ \\\\\n",
    " Loss &= \\frac{1}{2}(||\\hat{y} - y||_2^2 + \\lambda ||W||_2^2) \\\\ \\\\\n",
    " W^* &= (X^TX+\\lambda I)^{-1}X^Ty\n",
    "\\end{align*}$$\n",
    "\n",
    "This Loss function is *regularized* Least Squared Error (LSE). The additional term $\\lambda ||W||_2^2$ quantifies the magnitude of W. That is, this loss function is larger for models with high magnitude weights, and as such the optimal solutions produced by this will be lower magnitude in general. The $\\lambda$ term is a hyperparameter which can be chosen to tune how much this loss function \"cares about\" the magnitude of the weights."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b83c4a2",
   "metadata": {},
   "source": [
    "1. Write a function to create an optimal d-degree polynomial for given data and regularization factor (lambda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eef264d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def regularized_poly_fit(train_data, test_data, degree, lam):\n",
    "    '''\n",
    "    modify your poly fit model to add regularization\n",
    "    '''\n",
    "    return opt_w, train_loss, test_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9260467",
   "metadata": {},
   "source": [
    "2. To determine a good setting to our hyperparameters we'll do a grid search over possible choices of degree and lambda and plot the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be20d495",
   "metadata": {},
   "outputs": [],
   "source": [
    "def grid_search(lambdas=[0, 0.25, 0.5, 0.75, 1], degrees=[1,2,3,4,5,6,7]):\n",
    "    '''\n",
    "    Train a model with each combination of the values in the lambdas and degrees lists\n",
    "\n",
    "    This should return 2 2d np arrays, 1 with training losses, the other with test losses. \n",
    "    '''\n",
    "\n",
    "    return train_losses, test_losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "052eb5cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot these matrices as seaborn heatmaps\n",
    "\n",
    "import seaborn as sn\n",
    "\n",
    "train_losses, test_losses = grid_search()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ea5d7a",
   "metadata": {},
   "source": [
    "3. Using this result and any other method of your choosing produce a single best model, we will then test this on newly sampled data to see how well it generalizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e749d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use this block to produce your best_model however you'd like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a75ae49b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing your best_model\n",
    "\n",
    "new_test_points = sample_polynomial(100)\n",
    "loss = 0 # calculate the LSE of your best_model on these new points\n",
    "\n",
    "print(loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
