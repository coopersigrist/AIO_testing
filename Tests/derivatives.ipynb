{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "510b476e",
   "metadata": {},
   "source": [
    "# Derivatives Test\n",
    "\n",
    "This is somewhat harder as per request. Sections 1 and 2 are to be done during \"class\", finishing and the plotting will be done as homework, though times are still provided for pacing. It is likely that no one will finish in class but I still expected every team to have perfect solutions next week."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "277d2d5e",
   "metadata": {},
   "source": [
    "## Section 1 : 1D derivatives (20 min)\n",
    "\n",
    "The following derivatives identities may come in handy: \n",
    "$\\begin{align}\n",
    "sin(x) \\frac{d}{dx} = cos(x), \\quad cos(x) \\frac{d}{dx} = -sin(x), \\quad ln(x) \\frac{d}{dx} = \\frac{1}{x}\n",
    "\\end{align}$\n",
    "<br />\n",
    "<br />\n",
    "(Hint: 4 and 5 are easiest with \"implicit differentiation\" try taking the derivative w.r.t x without rearranging)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce28cb8",
   "metadata": {},
   "source": [
    "1. $y = (x^2 + 1)^7  \\quad \\frac{dy}{dx} = $\n",
    "<br />\n",
    "<br />\n",
    "\n",
    "2. $\\sigma (x) = \\frac{1}{1 + e^{-X}} \\quad \\frac{d\\sigma (x)}{dx} = $\n",
    "<br />\n",
    "<br />\n",
    "\n",
    "3. $y = ln(sin^2(x^3)) \\quad \\frac{dy}{dx} = $\n",
    "<br />\n",
    "<br />\n",
    "\n",
    "4. $xy + x^2 = y^2 \\quad \\frac{dy}{dx} = $\n",
    "<br />\n",
    "<br />\n",
    "\n",
    "5. (*) $e^{xy} + y^2 = cos(x) + 1 \\quad \\frac{dy}{dx} = $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049219ae",
   "metadata": {},
   "source": [
    "## Section 2 : Matrix Derivatives (35 min)\n",
    "\n",
    "For this section we will do derivatives on the following linear model with inputs X, labels y, weight matrix W and bias b:\n",
    "\n",
    "$\\begin{align}\n",
    " \\hat{y} &= X \\cdot W + b \\\\ \\\\\n",
    " Loss &= \\frac{1}{2}(||\\hat{y} - y||_2^2 + \\lambda ||W||_2^2)\n",
    "\\end{align}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a910ed27",
   "metadata": {},
   "source": [
    "1. Find the gradient of the Loss (regularized LSE) of this model:\n",
    "</br></br>\n",
    "$\\frac{\\partial Loss}{\\partial b} =$ \n",
    "</br> </br>\n",
    "$\\frac{\\partial Loss}{\\partial W} =$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf946ed1",
   "metadata": {},
   "source": [
    "2. Find the optimnal (W* and b*) parameter settings to minimize loss (hint: this can be done in closed form using basic calculus principles)\n",
    "</br></br>\n",
    "W* = \n",
    "</br></br>\n",
    "b* = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4367d73",
   "metadata": {},
   "source": [
    "3. Why is it possible to solve this in closed form? Is it possible with every model/ Neural Network? Why or why not?\n",
    "</br></br>\n",
    "Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a753934f",
   "metadata": {},
   "source": [
    "4.  Now consider a new model: $\\hat{y} = ((X \\cdot W_0 + b_0) \\cdot W_1 + b_1) \\cdot W_2 + b_2$, find the following derivatives:\n",
    "</br> </br>\n",
    "$\\frac{\\partial Loss}{\\partial W_2} =$ \n",
    "</br> </br>\n",
    "$\\frac{\\partial Loss}{\\partial W_0} =$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c902db8",
   "metadata": {},
   "source": [
    "5. (*)  Consider now adding a *nonlinearity*, $\\sigma$ defined the same as the function from (2) in the 1D section performed *elementwise* on matrices and alter out model like so:\n",
    "</br> </br>\n",
    " $\\hat{y} = \\sigma(\\sigma(X \\cdot W_0 + b_0) \\cdot W_1 + b_1) \\cdot W_2 + b_2$, find the following derivatives:\n",
    "</br> </br>\n",
    "$\\frac{\\partial Loss}{\\partial W_0} =$ \n",
    "</br> </br>\n",
    "$\\frac{\\partial Loss}{\\partial b_0} =$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ebbbd2",
   "metadata": {},
   "source": [
    "6. Are the models in (4) and (5) able to optimized to minimize LSE? Is so explain how, if not explain why not"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "095bef15",
   "metadata": {},
   "source": [
    "## Section 3 : Plotting / Implementation (20 min)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ccbe69",
   "metadata": {},
   "source": [
    "1. Write code to generate a dataset of 100 points X ~ 3 * U[0,50] + 8 and plot it as a scatter plot with all points being green. (U[0,50] is the uniform distribution between values 0 and 50.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47dccd7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def gen_and_plot():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca989cb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_and_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30995e20",
   "metadata": {},
   "source": [
    "2. Use your optimization from Matrix Derivatives (2) to create a model a linear model that is optimal to fit this and plot it alongside scatter of points, also in green. Report the values of W and b as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654a0799",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3ff38b41",
   "metadata": {},
   "source": [
    "3. Create two other datasets from the same distribution but with added Gaussian noise (sampled from a normal distribution) with mean 0 and standard deviation 5 and 10 respectively. Plot these along with their lines of best fit in blue and red respectively (each dataset/line pair are a different color). Make sure that there is a legend with one entry per dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45823847",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c1476d7d",
   "metadata": {},
   "source": [
    "4. (*) Sample data uniformly (non-randomly, i.e. grid-search) from Y ~ 3x1 + 5x2 + 3x1x2 - x2^2 for x1, x2 $\\in [0,99]$ (integer values, 10,000 points). Plot both the 3d data as well as a 2d plot of its gradient's magnitude."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2de500c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
